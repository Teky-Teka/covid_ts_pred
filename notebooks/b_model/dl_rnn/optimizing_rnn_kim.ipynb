{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cac72ae-9be5-43d4-87d3-b2319a0f757b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# COVID-19 Mortality Prediction with Reccurent Neural Network Deep Learning Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e2fb42",
   "metadata": {},
   "source": [
    "\n",
    "### Dependencies importing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6d3cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto reload imported module every time a jupyter cell is executed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# covid_ts_pred package\n",
    "from covid_ts_pred.c_eng.engineering import *\n",
    "from covid_ts_pred.b_data_prep.ba_preproc.preprocessor import train_test_set, scale_country_index\n",
    "from covid_ts_pred.c_model.cc_mach_learn.sequencing import subsample_sequence_2, get_X_y_2\n",
    "from covid_ts_pred.c_model.cd_deep_learn.RNN_model import train_rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de425903-9365-4407-8ae8-9b20a6e41dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.express as px\n",
    "import seaborn as sns \n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import requests\n",
    "import pandas_profiling\n",
    "# from typing import overload\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8efd82a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad4771-e464-4eb1-86f5-c5c480cea8d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6e1591b-31c5-4a38-b0fb-4bfaec0924f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Data project directory\n",
    "# data_dir = '../data/raw_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adfb03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "760eea31",
   "metadata": {},
   "source": [
    "# Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fed3547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, figsize=(17,7))\n",
    "# plt.plot(          ['new_deaths']);\n",
    "# ax.set_title(\"Covid 19 calculation for different countries\", size=10)\n",
    "# ax.set_ylabel(\"Number of death cases\", size=10)\n",
    "# ax.set_xlabel(\"Date\", size=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8b3bb-c12b-43a3-9dda-7bbd905f3622",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122911e-70c8-4131-bf2f-8db270b3ac93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TENSORFLOW & RNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636afbb-aab0-4500-897b-311961289203",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recurrent Neural Network (sequences data) modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f081b-7ee6-405b-8e13-04bf267ac2e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Samples/Sequences, Observations, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6427dc3-25b3-4a5a-abba-20e92d67ab1b",
   "metadata": {},
   "source": [
    "X.shape = (n_SEQUENCES, n_OBSERVATIONS, n_FEATURES) and y = RNN(X) where $X_{i,j}^{t}$\n",
    "\n",
    "with $_{i}$ is the sample/sequence, $_{j}$ is the feature measured and  $^{t}$ is the time at which the observation is seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8151dc-1f0e-4981-91a0-c1164230af26",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "- **retrieve dataset** from Sumedha & Alberto\n",
    "\n",
    "    - **clean dataset**: \n",
    "        \n",
    "        - **drop first lines == 0** *(before Covid arrived)*\n",
    "        \n",
    "        - **check Nan**: \n",
    "- **strategy 1 country by country** sequences split as follow:\n",
    "\n",
    "- **strategy 2 one sequence per country**:\n",
    "    - **split X train, set** \n",
    "    - **Pad sequences**\n",
    "    - **create one csv per country**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32848eaa-a889-47ee-972c-e9f2e54214cc",
   "metadata": {},
   "source": [
    "## Training strategies:\n",
    "- Get NB dataset (cleaned) from Alberto & Sumedha\n",
    "- 1/ Indicator in precentage %\n",
    "- 2/ Indicator as categorical labels\n",
    "- Run same RNN model in parallel with Kim & Thomas\n",
    "- Identify best dataset\n",
    "- Parameters to fit:\n",
    "    - increase **nb of sequences**\n",
    "    - train series modulation (ex: [50, 150, 200, 300, 400 nb of days = n_obs]) < take time to compute\n",
    "    - **learning_rate** in Optimizer(parameters)\n",
    "    - model layers architecture (**simple** -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "        > LSTM\n",
    "        > Dense\n",
    "       (> LSTM\n",
    "        > LSTM\n",
    "        > Dense)\n",
    "     >> **try to overfit** the model with the loss (train over val) or (early_stopping)\n",
    "     >> **(X_val, y_val)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4cdbb-56aa-4522-84a6-833471a7b34a",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775435d-8b7a-4742-bcb2-a76e1c2ffb1c",
   "metadata": {},
   "source": [
    "# RNN model Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "648961ed-ee19-4342-b137-8f5029082404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 61, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "n_seq = 200 ## nb of sequences (samples)\n",
    "n_obs = 61 # maxi = 96 (stay around 70 or more test_split)\n",
    "n_feat = 20 #  X_train.shape[1] # 20 feature:\n",
    "n_pred = 10 # nb of days where we can predict new daily deaths\n",
    "n_seq, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245d337-90d7-47b1-a4ee-b508e8731c39",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Splitting\n",
    "\n",
    "Split the dataset into training, validation, and test datasplit the dataset into training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fe70c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 2, 1, 2, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_set('France', split_train=0.7, split_val=0.9)\n",
    "np.ndim(X_train), np.ndim(y_train), np.ndim(X_val), np.ndim(y_val), np.ndim(X_test), np.ndim(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4fba2-9562-49eb-9b94-7c5ab57fd163",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create sequences (`X`,`y`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e6437-7538-4c29-b8d9-d00d4d804064",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Generates an entire dataset of multiple subsamples with shape $(X, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1993c868-606c-4d31-ac8d-997bc85d63d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((61, 9), (10,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subsample sequence\n",
    "(X_sample, y_sample) = subsample_sequence_2(X_train, y_train, X_len=n_obs, y_len=n_pred)\n",
    "X_sample.shape, y_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb0927-e52b-4d41-8dcb-6ca6f204a1f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **get_X_y(df, n_sequences, length)**\n",
    "\n",
    "function to generates an entire dataset of multiple subsamples suitable for RNN, that is, $(X, y)$ of shape:\n",
    "\n",
    "```python\n",
    "X.shape = (n_sequences, length, n_features)\n",
    "y.shape = (n_sequences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9096e3f-477e-45ae-a9a2-bdb1c8163ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 40, 20)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq_val = n_seq // 5 # number of sequences in test set ?\n",
    "n_seq_test = n_seq // 10 # number of sequences in test set ?\n",
    "n_seq, n_seq_val, n_seq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1682240c-818e-4e96-9a2a-8b596cd235bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96, 9), (96,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3af4e7e-d602-4fcd-a36a-942ab4dc0b72",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20, 61, 9), (20, 10), 20, 61, 20)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = get_X_y_2(X_test, y_test, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_test)\n",
    "print('X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat')\n",
    "X_test.shape, y_test.shape, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "497196a7-2d6a-4663-82a9-c8873c4ad0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((40, 61, 9), (40, 10), 200, 40, 20, 61, 20)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, y_val = get_X_y_2(X_val, y_val, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_val)\n",
    "print('X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat')\n",
    "X_val.shape, y_val.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a422c04-e3ef-4daf-b870-060c559ad2bb",
   "metadata": {},
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_set('United States', split_train=0.7, split_val=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e2fa4-2cf1-4ad1-bd39-6b48cd2d478d",
   "metadata": {
    "tags": []
   },
   "source": [
    "X_train, y_train = get_X_y(X_train, y_train, X_test, y_test, X_len=n_obs, y_len=n_pred, n_seq_train=n_seq_test, X_val=X_val, y_val=y_val)\n",
    "X_train.shape, y_train.shape, n_seq, n_seq_val, n_seq_test, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7826c700-4273-4cd3-b7d7-4c3cca2b3582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 61, 9), (200, 10), 200, 61, 20)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = get_X_y_2(X_train, y_train, X_len=n_obs, y_len=n_pred, n_sequences=n_seq)\n",
    "X_train.shape, y_train.shape, n_seq, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2a397-4b64-4358-8355-2a243e990afa",
   "metadata": {
    "tags": []
   },
   "source": [
    "X_val, y_val = get_X_y_2(X_val, y_val, X_len=n_obs, y_len=n_pred, n_sequences=n_seq_val)\n",
    "X_val.shape, y_val.shape, n_seq_val, n_obs, n_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe229ed-38be-485e-9314-5a3c04da8847",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### How to split sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49128bf-f5e8-4319-8635-fa1f63c29ceb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- randomly or\n",
    "\n",
    "- manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad867c99-9dd9-4dc5-9cda-de741bf57ca2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **train_rnn_model(model, patience=2, epochs=200):**\n",
    "\n",
    "function to generates an entire dataset of multiple subsamples suitable for RNN, that is, $(X, y)$ of shape:\n",
    "\n",
    "```python\n",
    "X.shape = (n_sequences, length, n_features)\n",
    "y.shape = (n_sequences, )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ece011-197d-439e-b405-ee2370c9773d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RNN model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f844bf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 61, 9)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a7bd6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20)                2400      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 2,720\n",
      "Trainable params: 2,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    " \n",
    "# Normalization layer\n",
    "normalizer = Normalization()  # Instantiate a \"normalizer\" layer\n",
    "normalizer.adapt(X_train) # \"Fit\" it on the train set\n",
    "\n",
    "# 1. The Architecture\n",
    "\"\"\"   - model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model = Sequential()\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model.add(LSTM(units=20, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# output return sequences = True\n",
    "rnn_model.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model.add(Dense(n_pred, activation = 'linear'))\n",
    "\n",
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8cd997f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxJElEQVR4nO3deXxU9b3w8c93sm9kDySZJIPIIoIsiQgKVuuO3qqIVmqt3nprvdfb2t6lt+19nnuf5bZPb5/b+9yutlrsghYr4tZKXbooVVlMABUSdsjGMiEJgYTs83v+ODMhhARCZjkzJ9/36zWvmTkzOfNlgO85+f6+5/cTYwxKKaWcxWV3AEoppUJPk7tSSjmQJnellHIgTe5KKeVAmtyVUsqB4u0OACAvL894PB67w1BKqZhSVVV1zBiTP9xrUZHcPR4PlZWVdoehlFIxRURqR3pNyzJKKeVAmtyVUsqBNLkrpZQDaXJXSikH0uSulFIOpMldKaUcSJO7Uko5kCZ3pdT4c2Q7HFhvdxRhpcldKTX+/P5f4bkHwOezO5Kw0eSulBp/jlZDZws01dgdSdhocldKjS+drXDykPX44Dv2xhJGmtyVUuOLd+fpxw6uu2tyV0qNL95q637y1VD7rmPr7prclVLjS9NOSMyAOSusEk0g2TuMJnel1PjirYGCGeBZbD13aN1dk7tSavwwBo7ugIJLIKsUssrg4J/tjiosNLkrpcaPjiarBbJgpvXcs8Q6c3dg3V2Tu1Jq/AjU1wsuse4nL4Gu4+DdYVtI4aLJXSk1fgTaIPP9yb3sKuvegXV3Te5KqfHDWw0pOZBeYD3PKoFsDxxwXt1dk7tSavzw1lj1dpHT2zyLHdnvrsldKTU+GGP1uAfq7QGeq626+9HttoQVLudN7iLylIh4RWT7oG3/V0R2isiHIvKiiGQNeu1rIrJXRHaJyE1hilsppS7MiUboPjFMcndm3X00Z+4/B24esu1NYJYx5jJgN/A1ABGZCdwLXOr/mR+JSFzIolVKqbHy+meAHJrcM92QPdlx/e7nTe7GmPVAy5Btbxhj+vxPNwJu/+PbgWeNMd3GmAPAXmBBCONVSqmxCbRB5s84+7XJS/x19/7IxhRGoai5fxb4nf9xMVA/6LUG/7aziMjDIlIpIpVNTU0hCEMppc7BWwMZhZCac/ZrniXQ1eaountQyV1E/hnoA5650J81xjxhjKkwxlTk5+cHE4ZSSp2ft/rskkyAA/vdx5zcReRB4DbgPmOM8W9uBEoGvc3t36aUUvbx9UPT7tPTDgyVWQw5Fzmq331MyV1Ebga+AnzCGHNq0EuvAPeKSJKITAamApuDD1MppYLQehD6Ooevtwd4lkDte46pu4+mFXI1sAGYLiINIvIQ8AMgA3hTRLaJyI8BjDE7gOeAauA14FFjjDO+KaVU7BrolBnhzB2s5N7dBkc+ikxMYRZ/vjcYY1YMs3nlOd7/DeAbwQSllFIhFUju+dNHfs9Av/ufoWhu2EMKN71CVSnlfN5qa+72pPSR3zOhCHKmOGZQVZO7Usr5mnaeuyQTMNk5dXdN7kopZ+vrgWO7raX1zsezxJqi4MiH4Y8rzDS5K6WcrWUf+PpGd+YeWFfVAS2RmtyVUs42dPWlc8mYBLlTHVF31+SulHI2bw1InJW0R8OzGOo2QH/f+d8bxTS5K6WczVsDuVMgIXl07/csdkTdXZO7UsrZvDXnvjJ1qEDdPcanANbkrpRyrt5OaNk/usHUgIxJkDct5uvumtyVUs7VtAswoxtMHcyzGGpju+6uyV0p5VyjmVNmOJ7F0HMSjnwQ+pgiRJO7Usq5mmogLtGazvdClMV+v7smd6WUc3lrrPp53HnnSDxTxkTImx7TdXdN7kop5/LWXHi9PSDG+901uSulnKnrBLTVB5fce9rh8LaQhhUpmtyVUs7UtNO6v9DB1IAY73fX5K6UcqaBTpkxnrmnF1gXP8Vo3V2Tu1LKmbw1kJAKmaVj34dnMdRthP7e0MUVIZrclVLO5K22zrxdQaS5QN390LaQhRUpo1kg+ykR8YrI9kHb7haRHSLiE5GKQds9ItLpXzR7YOFspZSKOG/N2OvtAWWxW3cfzSHt58DNQ7ZtB5YB64d5/z5jzFz/7ZEg41NKqQvX0Qwd3rHX2wPS8yH/kpisu583uRtj1gMtQ7bVGGN2hS0qpZQKRlOQg6mDxWjdPRw198kislVE3haRJSO9SUQeFpFKEalsamoKQxhKqXEr2E6ZwTyLobcDDm0Nfl8RFOrkfhgoNcbMA/4O+JWITBjujcaYJ4wxFcaYivz8/BCHoZQa17zVkJwJGYXB7ytG+91DmtyNMd3GmGb/4ypgHzAtlJ+hRvbi1gYee3Yrxhi7Q1HKXoHBVJHg95WWZ+0rxuruIU3uIpIvInH+xxcBU4H9ofwMNbx+n+E/Xt/Ny9sO8UFDm93hKGUfY6wz91CUZAJisO4+mlbI1cAGYLqINIjIQyJyp4g0AIuAV0Xkdf/brwY+FJFtwPPAI8aYlmF3rELqrV1eGo93AvB8Vb3N0Shlo5NHoKst+DbIwTyLofcUNG4J3T7D7LzzYBpjVozw0ovDvHctsDbYoNSFW7WxloKMJC6fnMMr2w7x326dSXJCnN1hKRV53mrr/kLWTT2fwf3upVeEbr9hpFeoOkBtcwdv725ixYJSVlxeyomuPt6sPmp3WErZI5SdMgFpuVBwaUzV3TW5O8CvNtXhEmHFglKunJJLUWYya6oa7A5LKXt4ayCtwBoIDSXPYqjfBH09od1vmGhyj3Fdvf38urKeG2dOZFJmMi6XcFe5m3f2NHGkrcvu8JSKvFAPpgZMXmLV3Q/FRt1dk3uMe/XDwxw/1cv9C8sGti0vd+MzsHaLnr2rccbng6ZdoR1MDSi7yrqPkX53Te4xbtXGWi7KT2PRlNyBbWW5aSzw5LC2qkF73tX40lZnXU1aEMLB1IDUHJg4K2bq7prcY9j2xja21R/n/oVlyJCLNZZXuNl/rIMtda02RaeUDQYGU8Nw5g7+fvdN0Ncdnv2HkCb3GPb0xlpSEuJYNt991mtLZxeSkhDH8zqwqsaTcLRBDuZZAn2dMdHvrsk9RrV19vLStkbumFdEZkrCWa+nJ8WzdHYhv/ngMJ09/TZEqJQNvDshswSSh53SKnhlVwISE6UZTe4xam1VA129Pu67omzE9ywvd9Pe3cfrO45EMDKlbOStCd9ZOwyqu0f/oKom9xhkjOHpjbXMK81iVnHmiO+7YnIOJTkprNHpCNR40N8Hx3aFpw1ysIF+9+iuu2tyj0Hv7Wtm/7GOM9ofh+NyCXfNd/PevmYaWk9FKDqlbNKyH/p7wjeYGjB5CfR1QWNVeD8nSJrcY9CqDbVkpyawdPb556q+a74bY+CFLY0RiEwpGwUGU8N95l66iFiou2tyjzGH2zp5s+Yo91xeMqqJwUpyUll0US7Pa8+7crqmnYBA/vTwfk5qDkyK/rq7JvcYs3pzPT5juG/BuUsyg91d4aau5RSbD+jsy8rBvNWQMxkSUsL/WZ6roX5zVNfdNbnHkN5+H89uruOaafmU5qaO+udunjWJ9KR47XlXzhZYfSkSPIutuntDZWQ+bww0uceQN6uP4j3Zzf2LRn/WDpCaGM+tswt59aPDdHT3hSk6pWzU2wXN+8Jfbw8oi/66uyb3GLJqQy3u7BQ+Nq3ggn92eYWbUz39/G679rwrB2reA6Y/csk9JRsmzY7qursm9xix13uSDfub+dQVpcS5LnzR34qybDy5qayp1J535UDendZ9pMoyAJP9dffe6JxaW5N7jHh6Yx2JcS7uqSgZ08+LCMvL3Ww60EJds/a8K4fxVoMrHnKmRO4zPYuhvxsao7PuPpoFsp8SEa+IbB+07W4R2SEiPhGpGPL+r4nIXhHZJSI3hSPo8aaju4+1VQ0snT2JvPSkMe9n2Xw3IvC8zvOunMZbA7lTIT4xcp8Z5f3uozlz/zlw85Bt24FlwPrBG0VkJnAvcKn/Z34kIrpKc5Be3naIk919FzyQOlRRVgqLL85jbVUDPp/2vCsHCdfqS+eSkgWFl8VucjfGrAdahmyrMcbsGubttwPPGmO6jTEHgL3AgpBEOk4ZY/jlhoNcUjiB+aXZQe9vebmbxuOdbNzfHILolIoC3e1wvDay9fYAz5KorbuHuuZeDAwesWvwbzuLiDwsIpUiUtnU1BTiMJxjS10rO4+cHHZBjrG46dJJZCRrz7tykGP+88xIn7mDldz7u6Hh/ch/9nnYNqBqjHnCGFNhjKnIz8+3K4yot2pDLRlJ8dw+tygk+0tOiOMv5hSxbvthTnb1hmSfStlqYPUlG5J72SIQV1SWZkKd3BuBwe0cbv82NQbN7d2s++gId5W7SUuKD9l+l5e76er1se6jwyHbp1K28dZAfDJkeyL/2cmZUDgnKvvdQ53cXwHuFZEkEZkMTAU2h/gzxo3nKhvo6ffx6YWlId3vvJIspuSnsaZSSzPKAbzV1mRhLpt6NzyLrbJMb6c9nz+C0bRCrgY2ANNFpEFEHhKRO0WkAVgEvCoirwMYY3YAzwHVwGvAo8YYXeNtDPp9hmc21bLwohwuLsgI6b6tnvcSKmtbOXCsI6T7ViriIjmnzHA8S6x55KOs7j6abpkVxphCY0yCMcZtjFlpjHnR/zjJGDPRGHPToPd/wxgzxRgz3Rjzu/CG71xv7/bS0NrJ/Qs9Ydn/svnFuASe11WaVCzrbIWTh+2ptweULozKurteoRqlVm2oJT8jiRsvnRiW/U+ckMzV0/J5YUsj/drzrmJVYNqBfBuTe3ImFM6FA9FVd9fkHoXqmk/x1u4mViwoJSEufH9Fd5eXcLiti3f3HgvbZygVVpFafel8PIutaQh6omdqD03uUeiZzbW4RFixYGzzyIzWdZcUkJmSoD3vKnZ5ayAxAzLd9sYRhXV3Te5Rpqu3n+fer+eGSyZSmBneFWWSE+K4fW4Rr+84Qlun9ryrGOStsc7aQ3CBX1BKF4LERVXdXZN7lFn30WFaT/UGPY/MaC0vd9Pd5+O3Hx6KyOcpFTLG2DOnzHCSJ0DR3Kjqd9fkHmVWbazlovw0rpySG5HPm12cyfSJGdrzrmJPRxN0tkRHcgd/v3v01N01uUeR7Y1tbK07zqevCM08MqMRmOd9W/1x9npPRuQzlQqJaBlMDfAsAV8vNETHdZua3KPIM5tqSU5wcVd5ZAeH7phXTJxLWKMDqyqWDMwpY+MFTINFWd1dk3uUaOvs5aWth7h9TjGZKQkR/ez8jCSunZ7Pi1sa6ev3RfSzlRozbzWk5kJalEw8mJQBRfOipt9dk3uUeGFLA529/REbSB1qeXkJ3pPd/HmP9ryrGOHdaZ21290pM5hnMTRWQY/903poco8CxhhWbaxlbkkWs4ozbYnh4zMKyElL1J53FRuMscoy+TPsjuRMgbp7vf11d03uUWDDvmb2N3Vw/0J7ztoBEuNd3D63iDerj3L8VI9tcSg1Km0N0HMyegZTAwbq7vaXZjS5R4FVG2vJSk3g1ssKbY1jebmbnn4fr3ygPe8qykXbYGpAUjoUz4+KQVVN7jY70tbFG9VH+WRFCckJ9q4lfmlRJjMLJ2jPu4p+A22QUVaWgaipu2tyt9nqzXX4jOFTV4R2QY6xWl7u5qPGNnYeOWF3KEqNrGknZBRBSvCLxoecZzH4+qB+k61haHK3UW+/j2ffr+Nj0/Ipy02zOxzA6nlPiBOe17N3Fc281dF51g5QshBc8ba3RGpyt9Hvq49y9EQ3n77CvoHUoXLSEvn4jAJe2tZIr/a8q2jk64emXdFXbw9ISoci++vumtxttGpjLcVZKVw7o8DuUM5wd3kJx9p7eGtXk92hKHW21oPQ1xV9nTKDeRbDoS3Q3W5bCJrcbbLXe5L39jXzqStKiXNF0UUYwMem55OXnqhL8KnoNNApE+XJ3ea6+2gWyH5KRLwisn3QthwReVNE9vjvs/3brxGRNhHZ5r/9SziDj2VPb6wjIU745OXhXZBjLBLiXNw5r5g/1Hhpbu+2OxylzhRI7tF2AdNgpf66u4397qM5c/85cPOQbV8F/mCMmQr8wf884M/GmLn+2/8KTZjOcqqnj7VVDSydXUheepLd4QxreXkJfT7Dy9u0511FGW81ZJVBYnQ0IQwrMQ2Ky22tu583uRtj1gMtQzbfDvzC//gXwB2hDcvZXt52iJPdfbZekXo+0ydlcJk7U2eKVNHHWxO9g6mDeRZDo31197HW3CcaYw77Hx8BJg56bZGIfCAivxORS4MLz3mMMazaUMuMSRmUl0Vhj+4gy8vd1Bw+wY5DbXaHopSlrwea90R3vT3AswRMP9RvtOXjgx5QNcYYwPifbgHKjDFzgO8DL430cyLysIhUikhlU9P46crYUnec6sMnuH9R5BbkGKtPzCkiMc6lV6yq6NGyzxqojIUz95IF4Eqwrd99rMn9qIgUAvjvvQDGmBPGmHb/43VAgojkDbcDY8wTxpgKY0xFfn6UzMccAU9vrCU9KZ475hbbHcp5ZaUmcsPMiby8rZGePu15V1Eg2lZfOheb6+5jTe6vAA/4Hz8AvAwgIpPEfzoqIgv8+28ONkinaOno4dUPD7NsfjFpSfF2hzMqyyvctJ7q5Y87j9odilJWvV3iIG+q3ZGMjmcxHNoK3ZFfwnI0rZCrgQ3AdBFpEJGHgG8BN4jIHuB6/3OA5cB2EfkA+B5wr79so4DnKuvp6ffx6SgeSB1qycV5FGQk6TzvKjp4ayB3CsRHZ5fZWSb76+51ke93P+/pozFmxQgvXTfMe38A/CDYoJyo32d4ZlMtV0zOYdrEDLvDGbX4OBfL5rt58s/78Z7soiAj2e6Q1HjmrYZJs+2OYvTc/rr7wfUw9fqIfrReoRoh63c3Ud/SadsyesFYXu6m32d4eav2vCsb9XZCy4HYGEwNSEwFd4UtdXdN7hGyamMt+RlJ3Dhzkt2hXLCLC9KZV5rFmqp6tMqmbNO0CzCxMZg6mGcxHNoGXZGdRluTewTUt5ziT7u8rLi8hMT42PzKl5e72X20nY8atedd2WRg2oFYS+6Buntk+91jM9PEmGc21eESYUWULMgxFrddVkRSvPa8Kxt5qyEuEXIusjuSC+O+3Io7wvPMaHIPs67efp6rrOf6SwoozEyxO5wxy0xJ4KZLJ/HKB4fo6u23Oxw1HnlrIG86xMVGG/GAxFQojnzdXZN7mP1u+2FaOnq4f6HH7lCCdneFm7bOXn5foz3vygZNO2Ov3h4weQkc3hbRursm9zBbtaGWyXlpXDkl1+5QgnbllDwKM5O1511FXtcJaKuP3eTuWQzGB3UbIvaRmtzDaMehNrbUHee+K0pxRdmCHGMR5xLumu9m/e4mjrR12R2OGk+adlr3sZrcbai7a3IPo6c31pGc4OLu8uhbkGOs7ip34zPw4tZGu0NR40kszSkznIQUK8FHsO6uyT1MTnT18tLWRj4xp4jM1AS7wwmZyXlpXO7J1p53FVneGkhIg8zY7TjDswQOfwBdkWkn1uQeJi9UNdDZ2++IgdShlpe72d/Uwdb643aHosYLbw0UzABXDKesQN29NjJ19xj+pqKXMYZVG2uZU5LFbHem3eGE3K2XFZGSEKc97ypyvDWxW5IJcF8OcUkRq7trcg+DDfub2dfUEdXL6AUjPSmeW2ZN4rfa864ioeMYdHhj78rUoRKSI1p31+QeBk9vrCUrNYHbLiu0O5SwWV7h5mR3H6/vOGJ3KMrpAtMOxPqZO1j97kc+hM7jYf8oTe4hdvREF6/vOMo9FSUkJ8TZHU7YLJycizs7RXveVfgNJPcYmg1yJBHsd9fkHmKrN9fR7zN8akEMj+qPgsvf8/7O3mMcOt5pdzjKyZpqIDkLMmJvRtWzFFf46+7hL81ocg+h3n4fqzfXcfW0fDx5aXaHE3bLy90YAy9s0bN3FUbeGuusPcoXlB+VhGRr4ewIDKpqcg+hP9Qc5eiJbscOpA5VkpPKwotyeL6qQXveVXgYY13AVDDD7khCx7MEDoe/7q7JPYRWbaylOCuFj88osDuUiFleXsLB5lNU1rbaHYpyopOHrYt+nFBvD/AsBgzUvhfWjxlVcheRp0TEKyLbB23LEZE3RWSP/z7bv11E5HsisldEPhSR+eEKPprs9bbz7t5mPnVFKXEOmEdmtJbOnkRaYhzPa8+7CodYn3ZgOO4KiE8Oe919tGfuPwduHrLtq8AfjDFTgT/4nwPcAkz13x4GHg8+zOj3zKZaEuKEeyqcM4/MaKQmxrN0diG//fAQp3r67A5HOY3XP2FYrPe4DxafFJG6+6hmvTfGrBcRz5DNtwPX+B//AngL+Cf/9l8aqwi7UUSyRKTQGHM4JBEP0tPdxYlm++cW7+738XrVLm6ZNZn8jCS7w4m4uytKWFPVwGvbj7BsvtvucJSTeGsgfSKkxf6U2WfwLIE/fRNOtUBqTlg+IpglTSYOSthHgIn+x8VA/aD3Nfi3hTy5H9yxiWmvfCLUux2T94D2Y5fB+k/A9FutXyOdMLo/Cpd7sinLTWVNZYMmdxVa3mrId9BgakCg7l63AWbcGpaPCMl6VcYYIyIX1C4hIg9jlW0oLR1bT3hu8RQ2Xfrfx/SzoZZj2ri47V34479Zt6wy6y9t+lIoXRR7S4NdABFh+Xw333lzN/UtpyjJSbU7JOUEPp81j/v8B+yOJPSKy0/X3aMwuR8NlFtEpBDw+rc3AoMLz27/tjMYY54AngCoqKgYUx9d7kQ3uXf/w1h+NHxOHIbdr8GudfD+Stj4I+sCjGk3wfRb4OLrISnD7ihDblm5m//8/W7WbmngS9dPszsc5QTHa6H3lLMGUwPik6DkirDW3YNphXwFCBxSHwBeHrT9M/6umYVAWzjq7VFrQiFU/CXctwa+sh/uWWUl9T1vwJoH4dsXwdN3wfs/hROH7I42ZIqzUrhqSh5rtzTg82nPuwqBgdWXHNQGOZhnCRzZbtXdw2C0rZCrgQ3AdBFpEJGHgG8BN4jIHuB6/3OAdcB+YC/wJPA3IY86ViSlw8xPwJ0/hn/YCw+ugwUPQ/M+ePXv4T8vgSeugbe/bf0lx/iFQMvL3dS3dLLpQHj+sapxJtAGmT/d3jjCJcz97qPtllkxwkvXDfNeAzwaTFCOFBcPnqus243/Bk27rNLNrnXWqPmfvgFZpVaNfvpSKLsS4mJrBaebLp1ERlI8z1c1sMgBC4Irm3lrILMEkifYHUl4FM+H+BSr7n7JbSHfvXNH+aKZiHU5dcEMWPJ3cPLo6Tp91c9h048hOROm3uiv098QE//AUxLjuG1OIS9tPcT/vP1S0pP0n5cKghMW6DiX+CSY80nImHj+945l92HZq7owGROh/AHr1tMB+/5kJfrdr8FHa8CVYM0DPX2plewzo7fdcHl5Cas317Omsp5l89ykJcURH6ezXKgL1N8Hx3bDxWcVB5zlL74btl1LNEz4VFFRYSorK+0OI/r4+qF+M+x6FXaug5Z91vZJl51us5w0O6r66Y0xXPefb7O/qWNgW2K8i/SkeNKS4khLjPc/jj+9Len0Nuvx6felBp4HXk+MH1fTO4xbTbvhh5fDnT+BOffaHU3UEpEqY0zFsK9pco8hTbtP1+nrNwPGqklOv8Vfp78K4hPtjpL9Te28f7CF9u5+Orr76Ojuo91/39EzdJv1vL2nb9TjySkJcacPAmccKE4fGALb0pOt+4zkwC1h4HlaYjwuPVBEpx0vwZoH4PProXCO3dFErXMldy3LxJL8adZt8Zeg3Qu7X7cS/ZZVsPkJSMqE0isgNc+6pDklC1KyISXHuk/NOf08MS1sZ/wX5adzUX76Bf2MMYbO3v4zE/6IB4S+sw4c3pNddBzrH3j9VM/513YVgfREK9Gn+xN/xsDBwHqckXTmaxn+19L9B4v0pHhHr7hlG28NiAvy9JqJsdLkHqvSC2D+/dat5xTsf8sq3xz6AI5WQ2cr9HaM/POuhLMTfko2pGb7n49wUEhICctBQURITYwnNTEeQnCNl89n6OixDhQnu3o50WUdBE529dLe1cfJLuvxye7Tj9u7+2jp6KGu+RQn/Nu6+3zn/azEONegA8Sgg0PS6d8WstMSyU1LJDc9kZy0RHLTkshJSyQxXscjhuWthuzJ1r83NSaa3J0gMRVmLLVug/V2Qddx6yKJzlb/zf/4jG2t1tWAh7Zaj/vOsWxeXNKQhJ89zPMc6+AzoQgyCm1p6XS5xH+2ncCkzOQx76enz0d7dx/tXX2c6Orl5KCDRODxiSEHjPbuPupbTp3xfKTrujKS48lLT/InfCv5BxL/4Md56YlkpyWSMF4Gp5t2OrtTJgI0uTtZQjIkTLrwtSd7O08n/XMeGI5Dy/7T2/q7h9mZWLP6ZRZbyX6C27rPLIYJ/lvGpKjt6U+Md5ETb51tj5XPZ2jr7KW5o4eWjh6a27tp7uihub2Hlo5ujnX00NLeQ23zKbbUtdLS0TPiwSAzJeHM3wDSk6znaYnkpCeRl5ZIjv+1nNTE2OxU6u2yLvSbeYfdkcQ0Te7qbAkp1m1C0eh/xphBB4UWaD8KbY3WFAsnGqz7pt1Wm2dP+5k/Ky7rADChyEr2me7TjycUWweC9EkxO/mayyVkp1ln3qPh8xmOd/bS0tFNc3uPdSDwHxRa/AeF5o5uDhzroPJgK62nRj4YZKX6DwZpSeRlJFKYmUJxVgpFWSm4s63HWakJSBR1XNG8B0y/nrkHKTb/t6joI2KVhxJTrWTM7JHf29VmJfu2Rjgx6NbWaF25u++PIxwAJp191j+h6PTBIIYPAIO5XGKdeaclcvEoVmzs9xmOn7J+KzjWbt23dHQPPG72HyR2HjnJH3d66eo9cxwhNTGOIn/CL/Yn/aKsZIqzUinOTmFiRlJkfwPw1lj3mtyDEvv/E1TsSc60biP95zUGuk+cfebf1mg99tbAnt+fPWAcOAAMLgEVzIDiCmt+Epczu1riXGKVZ9KTmHqeix2NMbSe6qWxtZPG4/5bayeH/I+3N7bR0tFz1v4nTUj2J/wUirOHHghSrIHwUPFWWwP+OVNCt89xSJO7ij4ipw8AE0eYEdAY/28A/gNAm/8AEPgt4Gg17HnTmjIWIDEdiuZZ61cWV1jzaU8ojNyfKUqInP6tYLY7c9j3dPb003j8dMIPJP+G4528f7CV33x4mP4hdaDs1ISBhF/sL/cEyj/F2SnkpiWOvvTj3Ql5U6Pimo1YpsldxSYRfx9/Fky8dPj3GGMNzDVWQkOldf/eD8DXa70+odhK8oGEXzTX6v8f51IS47i4IJ2LC4a/VqHfZzh6omvgANDg/y3g0PFODhzr4J29x866ziAp3jWQ+GcVZ/LYdVNHvj7AW239naigaHJXziUCeRdbt8Al7L1dcOTD08m+sQpqXvG/P86aO9xdbiV9h5dzxirOJQM1+uEYY3UHNbQOOftvs+4ff2sfmw+08ORnKs7uQuput9py598fgT+Js2lyV+NLQrK18nzJgtPbOo5ZST6Q8He8aM3OCZCYYZ3RB87u3RUX3lo6zogIWamJZKUmMqv47NLPuo8O8+Vfb2PZj97lZ3+5gMl5g35batpl3efrYGqwNLkrlZZnLYM47Sbruc9nTdIWSPYNlfDe98HXZ70+wW3Nxa3lnDFZOruQiROS+dwvK1n2o3d58jMVVHhyrBcDC3Rop0zQdOIwpUajtxOOfHRmwj9ea712RjnHf3afN03LOedR29zBgz97n8bjnXzn7jn8xZwieP2frbWHv96o398o6MRhSgUrIeXsck57k1XOCST77SOUc9yXWwujxyfZEXnUKstN44W/vpKHV1XyhdVbaWjt5BFvNaLjHCGhZ+5KhYrPB817z0z4R7db5ZysUrj2n2H23Zq4hujq7ecfn/+Q33xwiA/Sv0jGzBtwLfux3WHFBD1zVyoSXK7T0zLP9S873NsJB9Zba+S++Hl493tw/f+AqTdE1SIrdkpOiOO7n5zL1IweMiuP8WxdOrd19+kyjUEK6ppiEXlMRLaLyA4R+ZJ/2/8QkUYR2ea/LT3PbpRyroQUa6D2c2/BXSuti6p+dTf8/Faof9/u6KKGyyV8cbbVG/9GUw53/3gDR9q6bI4qto05uYvILOBzwAJgDnCbiFzsf/n/GWPm+m/rQhCnUrHN5YLZy+HRzbD0P+DYHlh5PTx7nzWhmhqYU+bh5bdS19zBHT98l+pDJ2wOKnYFc+Z+CbDJGHPKGNMHvA0sC01YSjlUfCIs+Bx8catVg9//NvzoCnjlC9b0CeOZtwaSJrBw7mWseeRKAO7+8Xu8vbvJ5sBiUzDJfTuwRERyRSQVWAqU+F/7WxH5UESeEpHs4X5YRB4WkUoRqWxq0r88Nc4kpcPHvgKPbYMFn4dtq+F78+DNf7GmTR6PvDWQPwNEmFk0gRcfvZLS3DQ++/P3Wb25zu7oYs6Yk7sxpgb4d+AN4DVgG9APPA5MAeYCh4HvjPDzTxhjKowxFfn5+WMNQ6nYlpYHt3wLvlBlLU7x7vfgu3Pgnf+yBmPHC2OsC5gGXbxUmJnCmkcWsfjiPL72wkd8+7Wd+EaauF6dJagBVWPMSmNMuTHmaqAV2G2MOWqM6TfG+IAnsWrySqlzyS6DZT+BR96Bkivg9/8K35sPW34J/X12Rxd+7V5rkZeCM2cBTU+KZ+UDFaxYUMqP3trHY7/eRlfv+Rc/V8F3yxT470ux6u2/EpHB86jeiVW+UUqNxqRZcN8aePBVa076V74Aj18JNb+1zm6dqmnkBTri41x8885ZfPWWGfzmg0Pcv3ITrUPmnFdnC3Z5lbUiUg38BnjUGHMc+LaIfCQiHwLXAl8O8jOUGn88i+Gvfg+ffBqMD359H6y8EQ6+a3dk4TGw+tLw8/eLCI98bArfXzGPDxraWPb4e9Q2dwz7XmXRK1SVinb9fbDtGXjr/8DJwzD1Jrj+X0eexz4WvfIF2PkqfGX/ed/6/sEWPvfLSlwiPPmZCsrLhu3ZGBfOdYVqDC6NrtQ4ExcP5Q/AF7ZYV7fWb4THr4IXH4HjDuki8daMeNY+1OWeHF78m6vISI7nU09u5HcfHQ5zcLFJk7tSsSIxFRZ/Gb64Da78Amx/Ab5fDq99HTqa7Y5u7Iyxlta7gGl+J+dZk45dWjSBv/nVFp5cv59oqEJEE03uSsWa1By48X/DF7fAZffApsfhe3Ph7f8LPTFYh25rgJ6TFzyHe256Er/63EKWzirkG+tq+JeXd9DX7wtTkLFHk7tSsSrTDbf/EP56A0y+Gv70b9aFUO//FPp77Y5u9M4zmHouyQlxfH/FPD5/9UWs2ljLw6uq6OgeB62jo6DJXalYVzAD7n0GPvsG5EyBV/8efrgAtq+1piGOdoHVl/JnjOnHXS7ha0sv4X/fMYu3dnm55ycbOHpCJx3T5K6UU5ReAX+5Dj71HMQnw/OfhSevhX1/sjuyc/PWQEYRpGQFtZv7F5ax8oHLOXCsgzt/+C47j4zvScc0uSvlJCLWFMOPvAN3/BhONcOqO+CXt8OhrXZHN7wh0w4E49oZBTz3+UX0G8Pdj2/gnT3HQrLfWKTJXSkncsVZC4b8bSXc9E04/CE8cS386f9EV6nG1w/Hdod0QexZxZm8+DdXUZydwoM/28xz79eHbN+xRJO7Uk6WkAyLHrVmn5xzL7z9LWuxkFMtdkdmaT0IfV1jGkw9l6Isa9KxRVNy+craD/nOG7vGXaukJnelxoPkTLjjcbjt/1nL/v3kY9FRpgkMphaMbTD1XDKSE3jqwcv5ZEUJ3//jXr786210942fScc0uSs1XohAxWfhs69Z89WsvAmqfmFvTIE2yDF2ypxPQpyLb901m3+8aTovbTvEZ1Zupu1UDLWJBkGTu1LjTXE5fH49lF0Jv/kivPyofXPHe6sh2wOJaWH7CBHh0Wsv5rv3zmVr3XGWPf4u9S2nwvZ50UKTu1LjUVoufHotXP2PsPVpa8bJlgORj8O7M+T19pHcPreYVQ8t4Fh7D3f+6F3e3t1ET18UDS6HmM4KqdR4t+s1ePFh6/Gyn8K0GyPzuX098M1CuOoxuO5fIvOZwL6mdh782WbqWzpJindxmTuT+WXZlJdmM78sm7z0pIjFEqxzzQqpyV0pZZ21P3c/HPkIrv4KXPNVq50ynI5Ww+OLrAPKZXeH97OGONnVyzt7jlFV20pVXSvbG9vo7bdyYVlu6kCiLy/LZtrEDOJcEtH4RutcyT0+0sEopaJQzmR46E1r6oL134bGSrhrpTVJWbgMdMqErsd9tDKSE7hldiG3zLYWjuvq7Wd7Yxtb6lqpqm1l/Z5jvLC1EbCW+ptbkjWQ7OeWZJGZkhDxmC+UJnellCUhxZqIrGQBrPtH+MnVcM8vrAHYcGjaCRIHeVPDs/8LkJwQR4UnhwqPdTAzxlDf0klVXQtbao9TVdvKD/64B5/xXwRckDGQ7OeXZjE5Lw2R6Dq717KMUupsjVvguc9A+1G45dtQ/qCV1ULp2fvg2B74282h3W+YtHf38UG9leiralvZWtfKiS5rBsqctETml2YN1O4vc2eRkhjmshZallFKXaji+Va75Nq/gt9+CRreh1u/Y53dh4q3GiZdFrr9hVl6UjxXXZzHVRfnAeDzGfY1tQ8k+6q6Vn5f4wUg3iXMLJrA/FLr7L68LJuirBB+d6MQ1Jm7iDwGfA4Q4EljzH+JSA7wa8ADHATuMca0nms/euauVJTy9cPb/27dJs2Ge1ZZ9flg9ZyCbxZZA7fXfDX4/UWJ1o4etta3DiT8D+rb6Oy1rootzExmfln2QMKfWTiBxPjgutHD0i0jIrOAZ4EFQA/wGvAI8DDQYoz5loh8Fcg2xvzTufalyV2pKLf7DXjhr6zHdz4B028Obn+HtsIT18A9v4SZtwcdXrTq7fex8/DJgYHaqtpWGo9bF4wlxbuY487i1ssKeeBKz5j2H66yzCXAJmPMKf+HvA0sA24HrvG/5xfAW8A5k7tSKspNu9Eq0/z6flj9SVjyD3Dt18feLundad1H6AImuyTEuZjtzmS2O3MggR9p6xpI9lvqWjnYHJ6lEYNJ7tuBb4hILtAJLAUqgYnGmMBy5EeAicP9sIg8jHWWT2lpaRBhKKUiItsDD70B6/4B/vwf0FhltUum5V74vrzVEJcE2SEo8cSYSZnJLJ1dyFJ/G2a4jLngY4ypAf4deAOrJLMN6B/yHgMMW/cxxjxhjKkwxlTk5+ePNQylVCQF2iX/4ntQ+57VLtlQdeH78dZA/jSI056OcAmqmm+MWWmMKTfGXA20AruBoyJSCOC/9wYfplIqqpQ/AA+9DuKCn90M76+ECxm/89ZAfuQvXhpPgkruIlLgvy/Fqrf/CngFeMD/lgeAl4P5DKVUlCqaB59/GyZfDa/+Hbz011YXzPl0tcGJBluuTB1Pgp0Vcq2IVAO/AR41xhwHvgXcICJ7gOv9z5VSTpSaA59aAx/7KnzwLKy8AVr2n/tnmnZZ9w4fTLVbUAUvY8ySYbY1A9cFs1+lVAxxueDar4G7wrro6SfXwLKfwPRbhn+/jXPKjCc6n7tSKjSm3mC1S+Z4YPW98If/ZV0ENZS3BhLTIbMk4iGOJ5rclVKhk10Gn30D5t0Pf/4OPL0MOo6d+R5vNeRPt874Vdjot6uUCq2EZLj9B/CJ70PtBn+75KAr0L01WpKJAE3uSqnwmP8Z66InVxw8dTNsftI6i+9o0sHUCNDkrpQKn6K58PDbMOVa68rW1fda2/XMPew0uSulwis1B1b8Gq75+unyjJ65h51e+6uUCj+XC675Jyi5HA5tg/Rhp5xSIaTJXSkVOVM+bt1U2GlZRimlHEiTu1JKOZAmd6WUciBN7kop5UCa3JVSyoE0uSullANpcldKKQfS5K6UUg4k5kLWPQxXECJNQG0Qu8gDjp33XeODfhdn0u/jNP0uzuSE76PMGJM/3AtRkdyDJSKVxpgKu+OIBvpdnEm/j9P0uziT078PLcsopZQDaXJXSikHckpyf8LuAKKIfhdn0u/jNP0uzuTo78MRNXellFJncsqZu1JKqUE0uSullAPFdHIXkZtFZJeI7BWRr9odj51EpERE/iQi1SKyQ0Qeszsmu4lInIhsFZHf2h2L3UQkS0SeF5GdIlIjIovsjslOIvJl//+T7SKyWkSS7Y4p1GI2uYtIHPBD4BZgJrBCRMbzwox9wN8bY2YCC4FHx/n3AfAYUGN3EFHiu8BrxpgZwBzG8fciIsXAF4EKY8wsIA64196oQi9mkzuwANhrjNlvjOkBngVutzkm2xhjDhtjtvgfn8T6z1tsb1T2ERE3cCvwU7tjsZuIZAJXAysBjDE9xpjjtgZlv3ggRUTigVTgkM3xhFwsJ/dioH7Q8wbGcTIbTEQ8wDxgk82h2Om/gK8APpvjiAaTgSbgZ/4y1U9FJM3uoOxijGkE/gOoAw4DbcaYN+yNKvRiObmrYYhIOrAW+JIx5oTd8dhBRG4DvMaYKrtjiRLxwHzgcWPMPKADGLdjVCKSjfVb/mSgCEgTkU/bG1XoxXJybwRKBj13+7eNWyKSgJXYnzHGvGB3PDa6CviEiBzEKtd9XESetjckWzUADcaYwG9yz2Ml+/HqeuCAMabJGNMLvABcaXNMIRfLyf19YKqITBaRRKwBkVdsjsk2IiJYNdUaY8x/2h2PnYwxXzPGuI0xHqx/F380xjjuzGy0jDFHgHoRme7fdB1QbWNIdqsDFopIqv//zXU4cIA53u4AxsoY0ycifwu8jjXa/ZQxZofNYdnpKuB+4CMR2ebf9nVjzDr7QlJR5AvAM/4Tof3AX9ocj22MMZtE5HlgC1aX2VYcOBWBTj+glFIOFMtlGaWUUiPQ5K6UUg6kyV0ppRxIk7tSSjmQJnellHIgTe5KKeVAmtyVUsqB/j/kePPxPGtuvwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Evaluating\n",
    "history = train_rnn_model(rnn_model, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, patience=3, epochs=200)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e336f527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4    30440.353516\n",
       "9    13852.416992\n",
       "1    12101.888672\n",
       "0     8906.962891\n",
       "2     6933.649902\n",
       "7     6583.358398\n",
       "5      410.263092\n",
       "8     -664.407227\n",
       "3    -7794.105469\n",
       "6   -22805.753906\n",
       "dtype: float32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Prediction\n",
    "y_pred = rnn_model.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a652e-f059-4adf-bf05-62b7120a24d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model 2 architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50d57380-7866-4ec5-8e40-b86653234646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_2 = Sequential()\n",
    "rnn_model_2.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_2.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_2.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_2.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_2.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b589e",
   "metadata": {},
   "source": [
    "## RNN model 2 training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e901218-eb1c-4d85-b40e-9beda6c3c548",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model 2 evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db1267f1-8a86-4f8b-82f5-01478bc0d350",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.361681\n",
       "3    0.181550\n",
       "5    0.136250\n",
       "2    0.091544\n",
       "6   -0.007438\n",
       "4   -0.048521\n",
       "8   -0.160029\n",
       "0   -0.257292\n",
       "7   -0.361496\n",
       "9   -0.508801\n",
       "dtype: float32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_2.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbe63f-114d-4717-a404-0aa7b96de1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model 2 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9524eb0a-0117-4006-af85-9f8235e96cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_1 (Normalizati (None, None, 9)           19        \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30)                4800      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 5,239\n",
      "Trainable params: 5,220\n",
      "Non-trainable params: 19\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_2.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739df8e1-5a54-4634-a074-e644a6a796a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "571254c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.callbacks.History"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(model=rnn_model_2, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, validation_split=0.1, patience=5, epochs=200, batch_size=16)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78dfb8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5    25.905685\n",
       "4    25.541424\n",
       "8    25.177368\n",
       "3    25.145010\n",
       "0    24.863075\n",
       "1    23.835806\n",
       "2    23.757242\n",
       "9    23.752163\n",
       "6    23.451788\n",
       "7    22.956247\n",
       "dtype: float32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_2.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1950356",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model 3, LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2ad1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_3 = Sequential()\n",
    "rnn_model_3.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_3.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model.add(Dense(20, activation = 'relu'))\n",
    "rnn_model_3.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_3.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae1226",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model 3 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47f878a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 30)                6120      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,581\n",
      "Trainable params: 6,540\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_3.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b737ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 13s 291ms/step - loss: 5914933760.0000 - mape: 99.9998 - val_loss: 19172208640.0000 - val_mape: 100.0000\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 5914905600.0000 - mape: 99.9995 - val_loss: 19172192256.0000 - val_mape: 99.9999\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 5914854912.0000 - mape: 99.9991 - val_loss: 19172169728.0000 - val_mape: 99.9999\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 5914783232.0000 - mape: 99.9983 - val_loss: 19172136960.0000 - val_mape: 99.9998\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 5914685440.0000 - mape: 99.9971 - val_loss: 19172132864.0000 - val_mape: 99.9998\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 5914581504.0000 - mape: 99.9958 - val_loss: 19172112384.0000 - val_mape: 99.9997\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5914464768.0000 - mape: 99.9944 - val_loss: 19172093952.0000 - val_mape: 99.9997\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 5914337792.0000 - mape: 99.9929 - val_loss: 19172075520.0000 - val_mape: 99.9996\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 5914209280.0000 - mape: 99.9910 - val_loss: 19172055040.0000 - val_mape: 99.9996\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 5914144768.0000 - mape: 99.9884 - val_loss: 19172034560.0000 - val_mape: 99.9995\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 5914049024.0000 - mape: 99.9846 - val_loss: 19172005888.0000 - val_mape: 99.9995\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5913893888.0000 - mape: 99.9802 - val_loss: 19172179968.0000 - val_mape: 99.9999\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 5913648128.0000 - mape: 99.9745 - val_loss: 19171835904.0000 - val_mape: 99.9990\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 5913331712.0000 - mape: 99.9705 - val_loss: 19171799040.0000 - val_mape: 99.9989\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5913126912.0000 - mape: 99.9666 - val_loss: 19171762176.0000 - val_mape: 99.9988\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 5912915456.0000 - mape: 99.9623 - val_loss: 19171731456.0000 - val_mape: 99.9987\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 5912692224.0000 - mape: 99.9582 - val_loss: 19171688448.0000 - val_mape: 99.9986\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 5912453632.0000 - mape: 99.9541 - val_loss: 19171645440.0000 - val_mape: 99.9985\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 5912205824.0000 - mape: 99.9492 - val_loss: 19171602432.0000 - val_mape: 99.9984\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 5911947264.0000 - mape: 99.9443 - val_loss: 19171565568.0000 - val_mape: 99.9983\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 5911677440.0000 - mape: 99.9395 - val_loss: 19171534848.0000 - val_mape: 99.9982\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 5911393792.0000 - mape: 99.9342 - val_loss: 19171518464.0000 - val_mape: 99.9982\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 1s 113ms/step - loss: 5911101440.0000 - mape: 99.9287 - val_loss: 19171516416.0000 - val_mape: 99.9982\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 5910799360.0000 - mape: 99.9229 - val_loss: 19171532800.0000 - val_mape: 99.9982\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 5910488576.0000 - mape: 99.9166 - val_loss: 19171557376.0000 - val_mape: 99.9983\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 1s 110ms/step - loss: 5910164480.0000 - mape: 99.9112 - val_loss: 19171586048.0000 - val_mape: 99.9984\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 5909819392.0000 - mape: 99.9047 - val_loss: 19171610624.0000 - val_mape: 99.9984\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 5909465088.0000 - mape: 99.8986 - val_loss: 19171643392.0000 - val_mape: 99.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(model=rnn_model_3, X_train=X_train, y_train=y_train, validation_split=0.1, patience=5, epochs=200, batch_size=16)\n",
    "\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c144bee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    3.755394\n",
       "9    3.462556\n",
       "8    2.883344\n",
       "4    2.827268\n",
       "7    2.684329\n",
       "1    2.472190\n",
       "5    2.268114\n",
       "0    1.761166\n",
       "2    1.683639\n",
       "6    0.890642\n",
       "dtype: float32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_3.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a652e-f059-4adf-bf05-62b7120a24d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RNN model 4 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50d57380-7866-4ec5-8e40-b86653234646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 3rd model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> LSTM\n",
    "\"\"\"\n",
    "rnn_model_4 = Sequential()\n",
    "rnn_model_4.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_4.add(LSTM(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_4.add(Dense(15, activation = 'relu'))\n",
    "rnn_model_4.add(Dense(5, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_4.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cbe63f-114d-4717-a404-0aa7b96de1b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile model 4 with 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9524eb0a-0117-4006-af85-9f8235e96cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 30)                6120      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 15)                465       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 80        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                60        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,766\n",
      "Trainable params: 6,725\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_4.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cc51a-a8a3-4a61-9c9e-96317f159d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d401da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 10s 293ms/step - loss: 5969966592.0000 - mape: 99.9998 - val_loss: 5419558400.0000 - val_mape: 99.9996\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 5969914368.0000 - mape: 99.9994 - val_loss: 5419499520.0000 - val_mape: 99.9993\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 5969830912.0000 - mape: 99.9989 - val_loss: 5419404800.0000 - val_mape: 99.9986\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 5969710080.0000 - mape: 99.9979 - val_loss: 5419279360.0000 - val_mape: 99.9973\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 5969557504.0000 - mape: 99.9958 - val_loss: 5419134464.0000 - val_mape: 99.9947\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 5969406464.0000 - mape: 99.9937 - val_loss: 5418984960.0000 - val_mape: 99.9923\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 5969246208.0000 - mape: 99.9917 - val_loss: 5418830848.0000 - val_mape: 99.9899\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 5969074176.0000 - mape: 99.9896 - val_loss: 5418648576.0000 - val_mape: 99.9872\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 5968875008.0000 - mape: 99.9873 - val_loss: 5418449920.0000 - val_mape: 99.9843\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 5968655360.0000 - mape: 99.9846 - val_loss: 5418235392.0000 - val_mape: 99.9813\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5968414208.0000 - mape: 99.9816 - val_loss: 5417996288.0000 - val_mape: 99.9779\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 5968223232.0000 - mape: 99.9784 - val_loss: 5417888768.0000 - val_mape: 99.9733\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 5967987200.0000 - mape: 99.9736 - val_loss: 5417432064.0000 - val_mape: 99.9675\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 5967675904.0000 - mape: 99.9696 - val_loss: 5417137152.0000 - val_mape: 99.9630\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 5967364608.0000 - mape: 99.9656 - val_loss: 5416790528.0000 - val_mape: 99.9576\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 5967005696.0000 - mape: 99.9610 - val_loss: 5416411648.0000 - val_mape: 99.9515\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5966628864.0000 - mape: 99.9555 - val_loss: 5416034304.0000 - val_mape: 99.9453\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5966228480.0000 - mape: 99.9503 - val_loss: 5415586816.0000 - val_mape: 99.9378\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5965778944.0000 - mape: 99.9443 - val_loss: 5415097344.0000 - val_mape: 99.9295\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 5965331456.0000 - mape: 99.9378 - val_loss: 5414985728.0000 - val_mape: 99.9219\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 1s 126ms/step - loss: 5964799488.0000 - mape: 99.9308 - val_loss: 5414466048.0000 - val_mape: 99.9124\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 5964293632.0000 - mape: 99.9219 - val_loss: 5413843968.0000 - val_mape: 99.8974\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 5963551232.0000 - mape: 99.8976 - val_loss: 5413241856.0000 - val_mape: 99.8866\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5962886656.0000 - mape: 99.8775 - val_loss: 5412610048.0000 - val_mape: 99.8754\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 5962888192.0000 - mape: 99.8636 - val_loss: 5412572160.0000 - val_mape: 99.8684\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5962081280.0000 - mape: 99.8507 - val_loss: 5411929088.0000 - val_mape: 99.8564\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 5961265152.0000 - mape: 99.8353 - val_loss: 5411270144.0000 - val_mape: 99.8442\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5960455168.0000 - mape: 99.8202 - val_loss: 5410460160.0000 - val_mape: 99.8298\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 5959612416.0000 - mape: 99.8033 - val_loss: 5409278976.0000 - val_mape: 99.8133\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5958536704.0000 - mape: 99.7870 - val_loss: 5408155136.0000 - val_mape: 99.7958\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5957313536.0000 - mape: 99.7643 - val_loss: 5407305728.0000 - val_mape: 99.7807\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5956350976.0000 - mape: 99.7426 - val_loss: 5406338560.0000 - val_mape: 99.7635\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 5955305472.0000 - mape: 99.7188 - val_loss: 5405331456.0000 - val_mape: 99.7455\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 5954211328.0000 - mape: 99.6946 - val_loss: 5404301312.0000 - val_mape: 99.7271\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 5953075200.0000 - mape: 99.6692 - val_loss: 5403208192.0000 - val_mape: 99.7076\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5951883264.0000 - mape: 99.6440 - val_loss: 5402068992.0000 - val_mape: 99.6874\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 5950979584.0000 - mape: 99.6223 - val_loss: 5400918528.0000 - val_mape: 99.6669\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5949480448.0000 - mape: 99.5978 - val_loss: 5399664640.0000 - val_mape: 99.6444\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5948003328.0000 - mape: 99.5702 - val_loss: 5398355968.0000 - val_mape: 99.6210\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5946583552.0000 - mape: 99.5403 - val_loss: 5396995072.0000 - val_mape: 99.5967\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 5945123328.0000 - mape: 99.5101 - val_loss: 5395649536.0000 - val_mape: 99.5728\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 5943622144.0000 - mape: 99.4826 - val_loss: 5394178048.0000 - val_mape: 99.5465\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5942034944.0000 - mape: 99.4496 - val_loss: 5392671744.0000 - val_mape: 99.5196\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5940408832.0000 - mape: 99.4161 - val_loss: 5391095296.0000 - val_mape: 99.4915\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 5938671104.0000 - mape: 99.3840 - val_loss: 5389463552.0000 - val_mape: 99.4625\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 5936928256.0000 - mape: 99.3458 - val_loss: 5387833856.0000 - val_mape: 99.4334\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5935112704.0000 - mape: 99.3134 - val_loss: 5386010112.0000 - val_mape: 99.4008\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 3s 279ms/step - loss: 5933175808.0000 - mape: 99.2739 - val_loss: 5384206848.0000 - val_mape: 99.3685\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 5931214336.0000 - mape: 99.2359 - val_loss: 5382372352.0000 - val_mape: 99.3357\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 5929199104.0000 - mape: 99.1943 - val_loss: 5380431872.0000 - val_mape: 99.3010\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 3s 231ms/step - loss: 5927085056.0000 - mape: 99.1558 - val_loss: 5378462720.0000 - val_mape: 99.2658\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 5924942848.0000 - mape: 99.1120 - val_loss: 5376372736.0000 - val_mape: 99.2285\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 5922699264.0000 - mape: 99.0638 - val_loss: 5374305280.0000 - val_mape: 99.1914\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5921228288.0000 - mape: 99.0239 - val_loss: 5372033536.0000 - val_mape: 99.1507\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5918166016.0000 - mape: 98.9722 - val_loss: 5369773056.0000 - val_mape: 99.1102\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 5915541504.0000 - mape: 98.9240 - val_loss: 5367606272.0000 - val_mape: 99.0714\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5913148928.0000 - mape: 98.8747 - val_loss: 5365287936.0000 - val_mape: 99.0299\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5910596096.0000 - mape: 98.8264 - val_loss: 5362699264.0000 - val_mape: 98.9835\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5907845632.0000 - mape: 98.7683 - val_loss: 5360205312.0000 - val_mape: 98.9389\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5905103360.0000 - mape: 98.7147 - val_loss: 5357616640.0000 - val_mape: 98.8926\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5902265856.0000 - mape: 98.6609 - val_loss: 5354877952.0000 - val_mape: 98.8437\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5899332608.0000 - mape: 98.6006 - val_loss: 5352093696.0000 - val_mape: 98.7938\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 5896290304.0000 - mape: 98.5452 - val_loss: 5349181952.0000 - val_mape: 98.7417\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5893208064.0000 - mape: 98.4753 - val_loss: 5346399232.0000 - val_mape: 98.6922\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5890135040.0000 - mape: 98.4183 - val_loss: 5343241216.0000 - val_mape: 98.6354\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5886785024.0000 - mape: 98.3527 - val_loss: 5340144640.0000 - val_mape: 98.5799\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5883480576.0000 - mape: 98.2812 - val_loss: 5337187328.0000 - val_mape: 98.5269\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5880174080.0000 - mape: 98.2156 - val_loss: 5333909504.0000 - val_mape: 98.4682\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5876623872.0000 - mape: 98.1464 - val_loss: 5330532864.0000 - val_mape: 98.4076\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5873005568.0000 - mape: 98.0815 - val_loss: 5327211520.0000 - val_mape: 98.3481\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 5869382656.0000 - mape: 98.0053 - val_loss: 5323684352.0000 - val_mape: 98.2847\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 5865557504.0000 - mape: 97.9297 - val_loss: 5320163328.0000 - val_mape: 98.2213\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5861729280.0000 - mape: 97.8489 - val_loss: 5316471296.0000 - val_mape: 98.1548\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5857758720.0000 - mape: 97.7712 - val_loss: 5312849920.0000 - val_mape: 98.0897\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5853790720.0000 - mape: 97.6906 - val_loss: 5308976128.0000 - val_mape: 98.0201\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 5849652224.0000 - mape: 97.6084 - val_loss: 5305152512.0000 - val_mape: 97.9513\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5845463552.0000 - mape: 97.5234 - val_loss: 5301122048.0000 - val_mape: 97.8783\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 5841114624.0000 - mape: 97.4353 - val_loss: 5296966656.0000 - val_mape: 97.8033\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 5836642304.0000 - mape: 97.3522 - val_loss: 5292728832.0000 - val_mape: 97.7266\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 5832034304.0000 - mape: 97.2549 - val_loss: 5288370176.0000 - val_mape: 97.6478\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5827406336.0000 - mape: 97.1621 - val_loss: 5284159488.0000 - val_mape: 97.5719\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5821553664.0000 - mape: 97.0653 - val_loss: 5279547392.0000 - val_mape: 97.4881\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 5816563200.0000 - mape: 96.9723 - val_loss: 5274944512.0000 - val_mape: 97.4046\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 5811565568.0000 - mape: 96.8650 - val_loss: 5270316032.0000 - val_mape: 97.3207\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5806502912.0000 - mape: 96.7610 - val_loss: 5265471488.0000 - val_mape: 97.2328\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5801270272.0000 - mape: 96.6592 - val_loss: 5260729344.0000 - val_mape: 97.1465\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5796067328.0000 - mape: 96.5566 - val_loss: 5255758336.0000 - val_mape: 97.0562\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5790691328.0000 - mape: 96.4424 - val_loss: 5250897408.0000 - val_mape: 96.9679\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5785270272.0000 - mape: 96.3327 - val_loss: 5245787648.0000 - val_mape: 96.8749\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 5779681792.0000 - mape: 96.2307 - val_loss: 5240358400.0000 - val_mape: 96.7761\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5773907456.0000 - mape: 96.1107 - val_loss: 5235140096.0000 - val_mape: 96.6809\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5768160256.0000 - mape: 95.9849 - val_loss: 5229976064.0000 - val_mape: 96.5870\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5762432512.0000 - mape: 95.8762 - val_loss: 5224226816.0000 - val_mape: 96.4817\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5756296192.0000 - mape: 95.7524 - val_loss: 5218805248.0000 - val_mape: 96.3827\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5750275072.0000 - mape: 95.6443 - val_loss: 5212825600.0000 - val_mape: 96.2731\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5743852544.0000 - mape: 95.5175 - val_loss: 5207124992.0000 - val_mape: 96.1688\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 5737618432.0000 - mape: 95.3950 - val_loss: 5201220608.0000 - val_mape: 96.0608\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5731057664.0000 - mape: 95.2749 - val_loss: 5194869760.0000 - val_mape: 95.9444\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5724302336.0000 - mape: 95.1439 - val_loss: 5188907008.0000 - val_mape: 95.8351\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 5717658112.0000 - mape: 95.0243 - val_loss: 5182503424.0000 - val_mape: 95.7179\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 5710751744.0000 - mape: 94.8913 - val_loss: 5176249344.0000 - val_mape: 95.6028\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5703933952.0000 - mape: 94.7478 - val_loss: 5169892864.0000 - val_mape: 95.4858\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5696905216.0000 - mape: 94.6182 - val_loss: 5163067392.0000 - val_mape: 95.3598\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 5689553920.0000 - mape: 94.4763 - val_loss: 5156529664.0000 - val_mape: 95.2397\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 5682372096.0000 - mape: 94.3488 - val_loss: 5149725696.0000 - val_mape: 95.1139\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5674916864.0000 - mape: 94.2199 - val_loss: 5142617600.0000 - val_mape: 94.9824\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 5667262976.0000 - mape: 94.0784 - val_loss: 5135686656.0000 - val_mape: 94.8542\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 5659667968.0000 - mape: 93.9402 - val_loss: 5128436736.0000 - val_mape: 94.7199\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 5651767808.0000 - mape: 93.8029 - val_loss: 5121135104.0000 - val_mape: 94.5845\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 5643818496.0000 - mape: 93.6639 - val_loss: 5113934848.0000 - val_mape: 94.4512\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5635805696.0000 - mape: 93.5389 - val_loss: 5106290688.0000 - val_mape: 94.3093\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 5627697152.0000 - mape: 93.3893 - val_loss: 5098672640.0000 - val_mape: 94.1677\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 2s 125ms/step - loss: 5619345408.0000 - mape: 93.2474 - val_loss: 5090755584.0000 - val_mape: 94.0203\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 5610799616.0000 - mape: 93.1094 - val_loss: 5082980864.0000 - val_mape: 93.8755\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 5602358272.0000 - mape: 92.9767 - val_loss: 5075099136.0000 - val_mape: 93.7282\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 5593675776.0000 - mape: 92.8477 - val_loss: 5066876416.0000 - val_mape: 93.5749\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 5584733696.0000 - mape: 92.7159 - val_loss: 5058732032.0000 - val_mape: 93.4220\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 5575844864.0000 - mape: 92.5811 - val_loss: 5050526720.0000 - val_mape: 93.2688\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 5566819328.0000 - mape: 92.4501 - val_loss: 5041837056.0000 - val_mape: 93.1055\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5557501952.0000 - mape: 92.3131 - val_loss: 5033367040.0000 - val_mape: 92.9464\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 5548186624.0000 - mape: 92.1655 - val_loss: 5024593408.0000 - val_mape: 92.7812\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5538625536.0000 - mape: 92.0331 - val_loss: 5015898112.0000 - val_mape: 92.6178\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 1s 117ms/step - loss: 5529134592.0000 - mape: 91.8941 - val_loss: 5007221760.0000 - val_mape: 92.4547\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5519629312.0000 - mape: 91.7557 - val_loss: 4997925888.0000 - val_mape: 92.2784\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 5509557760.0000 - mape: 91.6156 - val_loss: 4989124608.0000 - val_mape: 92.1116\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 5499773952.0000 - mape: 91.4849 - val_loss: 4979786240.0000 - val_mape: 91.9354\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 5489818112.0000 - mape: 91.3376 - val_loss: 4970384896.0000 - val_mape: 91.7569\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 5479443968.0000 - mape: 91.1927 - val_loss: 4960578560.0000 - val_mape: 91.5709\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 5449381376.0000 - mape: 90.9714 - val_loss: 4914297856.0000 - val_mape: 91.2524\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 1s 126ms/step - loss: 5429394944.0000 - mape: 90.7768 - val_loss: 4903409664.0000 - val_mape: 91.0573\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5417974272.0000 - mape: 90.6169 - val_loss: 4892711424.0000 - val_mape: 90.8655\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 5406482432.0000 - mape: 90.4695 - val_loss: 4881905664.0000 - val_mape: 90.6713\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 5394982400.0000 - mape: 90.3085 - val_loss: 4871045632.0000 - val_mape: 90.4760\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 5383233536.0000 - mape: 90.1505 - val_loss: 4859842048.0000 - val_mape: 90.2741\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 5371422720.0000 - mape: 89.9869 - val_loss: 4848993792.0000 - val_mape: 90.0783\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 5359615488.0000 - mape: 89.8336 - val_loss: 4837416960.0000 - val_mape: 89.8691\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 5347476480.0000 - mape: 89.6710 - val_loss: 4826045440.0000 - val_mape: 89.6632\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 5335268864.0000 - mape: 89.5066 - val_loss: 4814369280.0000 - val_mape: 89.4515\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5322792960.0000 - mape: 89.3411 - val_loss: 4802225152.0000 - val_mape: 89.2309\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 5310063616.0000 - mape: 89.1698 - val_loss: 4791003136.0000 - val_mape: 89.0267\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 5297668608.0000 - mape: 89.0129 - val_loss: 4779134464.0000 - val_mape: 88.8104\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 5285015040.0000 - mape: 88.8508 - val_loss: 4766522368.0000 - val_mape: 88.5802\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 5271720448.0000 - mape: 88.6742 - val_loss: 4754444288.0000 - val_mape: 88.3593\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 5258687488.0000 - mape: 88.4968 - val_loss: 4741483520.0000 - val_mape: 88.1218\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 5244967936.0000 - mape: 88.3361 - val_loss: 4728787968.0000 - val_mape: 87.8888\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 5231641088.0000 - mape: 88.1565 - val_loss: 4716509696.0000 - val_mape: 87.6630\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 5218337792.0000 - mape: 87.9711 - val_loss: 4703886336.0000 - val_mape: 87.4305\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 5204649472.0000 - mape: 87.8045 - val_loss: 4691004416.0000 - val_mape: 87.1927\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 5190894080.0000 - mape: 87.6362 - val_loss: 4677778432.0000 - val_mape: 86.9482\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5186214400.0000 - mape: 87.3443 - val_loss: 4664115200.0000 - val_mape: 86.6950\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5173230592.0000 - mape: 86.8565 - val_loss: 4651019264.0000 - val_mape: 86.4519\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 5153670656.0000 - mape: 86.5207 - val_loss: 4637359616.0000 - val_mape: 86.1978\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 5137783296.0000 - mape: 86.3467 - val_loss: 4623417856.0000 - val_mape: 85.9379\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 5118836736.0000 - mape: 86.1239 - val_loss: 4609374208.0000 - val_mape: 85.6756\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 5104754688.0000 - mape: 85.9008 - val_loss: 4595829248.0000 - val_mape: 85.4220\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 17s 2s/step - loss: 5089012736.0000 - mape: 85.7334 - val_loss: 4581921792.0000 - val_mape: 85.1612\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 5073945088.0000 - mape: 85.5211 - val_loss: 4567716352.0000 - val_mape: 84.8941\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 5058761216.0000 - mape: 85.3028 - val_loss: 4552777728.0000 - val_mape: 84.6127\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 47s 4s/step - loss: 5043071488.0000 - mape: 85.0836 - val_loss: 4538117120.0000 - val_mape: 84.3359\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 5027530752.0000 - mape: 84.8157 - val_loss: 4524098560.0000 - val_mape: 84.0706\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 5012066816.0000 - mape: 84.5756 - val_loss: 4509058560.0000 - val_mape: 83.7854\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 4996147200.0000 - mape: 84.2903 - val_loss: 4494341120.0000 - val_mape: 83.5056\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 4980295680.0000 - mape: 84.1542 - val_loss: 4479356416.0000 - val_mape: 83.2201\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 2s 150ms/step - loss: 4964187136.0000 - mape: 83.7127 - val_loss: 4463814144.0000 - val_mape: 82.9233\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 4950932992.0000 - mape: 84.0412 - val_loss: 4448270336.0000 - val_mape: 82.6257\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 1s 117ms/step - loss: 4931073024.0000 - mape: 83.5931 - val_loss: 4432778240.0000 - val_mape: 82.3284\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 4914568192.0000 - mape: 83.3370 - val_loss: 4416886784.0000 - val_mape: 82.0226\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 4897827328.0000 - mape: 83.0350 - val_loss: 4401598976.0000 - val_mape: 81.7277\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 1s 128ms/step - loss: 4881280512.0000 - mape: 82.7281 - val_loss: 4386262528.0000 - val_mape: 81.4312\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 4864563712.0000 - mape: 82.4270 - val_loss: 4370112000.0000 - val_mape: 81.1181\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 4847272960.0000 - mape: 82.1184 - val_loss: 4353816576.0000 - val_mape: 80.8014\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 4829914112.0000 - mape: 81.8268 - val_loss: 4338095104.0000 - val_mape: 80.4951\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 4812951552.0000 - mape: 81.6549 - val_loss: 4321591808.0000 - val_mape: 80.1726\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 4795094016.0000 - mape: 81.2398 - val_loss: 4304500224.0000 - val_mape: 79.8378\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 1s 90ms/step - loss: 4776956928.0000 - mape: 80.7555 - val_loss: 4288267520.0000 - val_mape: 79.5189\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 1s 121ms/step - loss: 4759491072.0000 - mape: 80.7363 - val_loss: 4271271168.0000 - val_mape: 79.1841\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 1s 95ms/step - loss: 4740993024.0000 - mape: 80.2506 - val_loss: 4254840064.0000 - val_mape: 78.8595\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 1s 127ms/step - loss: 4723090432.0000 - mape: 79.8258 - val_loss: 4237153536.0000 - val_mape: 78.5091\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 4704600064.0000 - mape: 79.4254 - val_loss: 4220546304.0000 - val_mape: 78.1791\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 4686653440.0000 - mape: 79.1860 - val_loss: 4202295296.0000 - val_mape: 77.8154\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 2s 165ms/step - loss: 4667337728.0000 - mape: 78.9986 - val_loss: 4185669632.0000 - val_mape: 77.4831\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 4649211392.0000 - mape: 78.5343 - val_loss: 4168280064.0000 - val_mape: 77.1345\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 2s 216ms/step - loss: 4630301696.0000 - mape: 78.1230 - val_loss: 4150299904.0000 - val_mape: 76.7729\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 3s 249ms/step - loss: 4611234816.0000 - mape: 77.7735 - val_loss: 4132361728.0000 - val_mape: 76.4111\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 4591806976.0000 - mape: 77.4710 - val_loss: 4114110464.0000 - val_mape: 76.0418\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 4572414976.0000 - mape: 77.0503 - val_loss: 4096936704.0000 - val_mape: 75.6932\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 2s 163ms/step - loss: 4553518592.0000 - mape: 76.8151 - val_loss: 4078513408.0000 - val_mape: 75.3181\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 4533806080.0000 - mape: 76.2328 - val_loss: 4060005120.0000 - val_mape: 74.9399\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 4513978880.0000 - mape: 75.9803 - val_loss: 4041854464.0000 - val_mape: 74.5679\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 4494066688.0000 - mape: 75.6459 - val_loss: 4023342336.0000 - val_mape: 74.1872\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 1s 118ms/step - loss: 4474366976.0000 - mape: 75.2664 - val_loss: 4004724736.0000 - val_mape: 73.8030\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 4453990400.0000 - mape: 74.9881 - val_loss: 3985463808.0000 - val_mape: 73.4042\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 4433559040.0000 - mape: 74.6764 - val_loss: 3965978112.0000 - val_mape: 72.9993\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 4412873728.0000 - mape: 74.1468 - val_loss: 3947461376.0000 - val_mape: 72.6131\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 2s 154ms/step - loss: 4392536064.0000 - mape: 73.9125 - val_loss: 3927900672.0000 - val_mape: 72.2037\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 4371551232.0000 - mape: 73.5634 - val_loss: 3908448256.0000 - val_mape: 71.7951\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 4350789632.0000 - mape: 73.1183 - val_loss: 3889894144.0000 - val_mape: 71.4039\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 4330358784.0000 - mape: 72.7432 - val_loss: 3870331648.0000 - val_mape: 70.9900\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 4309394432.0000 - mape: 72.8046 - val_loss: 3850118400.0000 - val_mape: 70.5606\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 4287428864.0000 - mape: 72.1406 - val_loss: 3830207232.0000 - val_mape: 70.1360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(model=rnn_model_4, X_train=X_train, y_train=y_train, validation_split=0.1, patience=2, epochs=200, batch_size=16)\n",
    "\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db66b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f12663b3790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(20, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9    756.453491\n",
       "5    753.715088\n",
       "2    719.322144\n",
       "4    712.748169\n",
       "0    691.983032\n",
       "8    674.045654\n",
       "3    653.919678\n",
       "7    651.653687\n",
       "6    637.685913\n",
       "1    624.480774\n",
       "dtype: float32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_4.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739df8e1-5a54-4634-a074-e644a6a796a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train model 5, LSTM X 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "474fd986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 61, 20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ae6e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 4th model layers architecture (simple -> complex) (less data -> more data) (print(loss) function check lecture)\n",
    "> GRU\n",
    "\"\"\"\n",
    "rnn_model_5 = Sequential()\n",
    "rnn_model_5.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_5.add(LSTM(units=30, activation='tanh', return_sequences=True, input_shape=(None, X_train.shape[1], X_train.shape[2])))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_5.add(LSTM(units=20, activation='tanh', return_sequences =True))\n",
    "rnn_model_5.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_5.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2c33a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, None, 30)          6120      \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, None, 20)          4080      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, None, 10)          210       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, None, 10)          110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,561\n",
      "Trainable params: 10,520\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_5.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b13f6eb0-e734-48c4-a455-49949ea62a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Incompatible shapes: [16,10] vs. [16,61,10]\n\t [[node sub\n (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/losses.py:1372)\n]] [Op:__inference_train_function_65713]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sub:\nIn[0] IteratorGetNext (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py:866)\t\nIn[1] sequential_4/dense_11/BiasAdd (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/layers/core/dense.py:210)\n\nOperation defined at: (most recent call last)\n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_25168/4090895566.py\", line 1, in <module>\n>>>     history = train_rnn_model(rnn_model_5, patience=5, epochs=200)\n>>> \n>>>   File \"/tmp/ipykernel_25168/3069528292.py\", line 10, in train_rnn_model\n>>>     history =  rnn_model_5.fit(X_train, y_train,\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 817, in train_step\n>>>     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 460, in update_state\n>>>     metric_obj.update_state(y_t, y_p, sample_weight=mask)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/metrics_utils.py\", line 73, in decorated\n>>>     update_op = update_state_fn(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/metrics.py\", line 177, in update_state_fn\n>>>     return ag_update_state(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/metrics.py\", line 725, in update_state\n>>>     matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/losses.py\", line 1372, in mean_absolute_percentage_error\n>>>     (y_true - y_pred) / backend.maximum(tf.abs(y_true),\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25168/4090895566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_model_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25168/3069528292.py\u001b[0m in \u001b[0;36mtrain_rnn_model\u001b[0;34m(rnn_model_5, patience, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m                     restore_best_weights = True)\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# The fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     history =  rnn_model_5.fit(X_train, y_train, \n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Auto split for validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [16,10] vs. [16,61,10]\n\t [[node sub\n (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/losses.py:1372)\n]] [Op:__inference_train_function_65713]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sub:\nIn[0] IteratorGetNext (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py:866)\t\nIn[1] sequential_4/dense_11/BiasAdd (defined at /root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/layers/core/dense.py:210)\n\nOperation defined at: (most recent call last)\n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/root/.pyenv/versions/3.8.12/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_25168/4090895566.py\", line 1, in <module>\n>>>     history = train_rnn_model(rnn_model_5, patience=5, epochs=200)\n>>> \n>>>   File \"/tmp/ipykernel_25168/3069528292.py\", line 10, in train_rnn_model\n>>>     history =  rnn_model_5.fit(X_train, y_train,\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/training.py\", line 817, in train_step\n>>>     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 460, in update_state\n>>>     metric_obj.update_state(y_t, y_p, sample_weight=mask)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/utils/metrics_utils.py\", line 73, in decorated\n>>>     update_op = update_state_fn(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/metrics.py\", line 177, in update_state_fn\n>>>     return ag_update_state(*args, **kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/metrics.py\", line 725, in update_state\n>>>     matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n>>> \n>>>   File \"/root/.pyenv/versions/lewagon/lib/python3.8/site-packages/keras/losses.py\", line 1372, in mean_absolute_percentage_error\n>>>     (y_true - y_pred) / backend.maximum(tf.abs(y_true),\n>>> "
     ]
    }
   ],
   "source": [
    "history = train_rnn_model(model=rnn_model_5, X_train=X_train, y_train=y_train, validation_split=0.1, patience=5, epochs=200, batch_size=16)\n",
    "\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.71704]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.7171 ]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]\n",
      "\n",
      " [[248.95366]\n",
      "  [511.51523]\n",
      "  [547.7171 ]\n",
      "  ...\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]\n",
      "  [553.5758 ]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(20, 61, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31858/2308850997.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Distribution of the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 )\n\u001b[1;32m    671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(20, 61, 1)"
     ]
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_5.predict(X_test) \n",
    "print(y_pred)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71da140",
   "metadata": {},
   "source": [
    "### Train model 6, GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105727e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 6th model layers architecture\n",
    "> GRU\n",
    "\"\"\"\n",
    "rnn_model_6 = Sequential()\n",
    "rnn_model_6.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_6.add(GRU(units=30, activation='tanh'))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "# output return sequences = True\n",
    "rnn_model_6.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_6.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4532805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 30)                4680      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                310       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,042\n",
      "Trainable params: 5,001\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_6.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b164948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 5s 129ms/step - loss: 6508211712.0000 - mape: 99.9973 - val_loss: 6164634624.0000 - val_mape: 99.9925\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 6508042240.0000 - mape: 99.9963 - val_loss: 6164467200.0000 - val_mape: 99.9919\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 6507843584.0000 - mape: 99.9952 - val_loss: 6164217856.0000 - val_mape: 99.9901\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 6507555328.0000 - mape: 99.9932 - val_loss: 6163867648.0000 - val_mape: 99.9878\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 6507193856.0000 - mape: 99.9904 - val_loss: 6163424256.0000 - val_mape: 99.9827\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 118ms/step - loss: 6506816000.0000 - mape: 99.9868 - val_loss: 6163078144.0000 - val_mape: 99.9791\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6506491904.0000 - mape: 99.9842 - val_loss: 6162754048.0000 - val_mape: 99.9760\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 6506182656.0000 - mape: 99.9819 - val_loss: 6162455040.0000 - val_mape: 99.9730\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 6505881088.0000 - mape: 99.9794 - val_loss: 6162149376.0000 - val_mape: 99.9700\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 6505568768.0000 - mape: 99.9768 - val_loss: 6161831936.0000 - val_mape: 99.9667\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6505258496.0000 - mape: 99.9743 - val_loss: 6161536512.0000 - val_mape: 99.9632\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6504960512.0000 - mape: 99.9716 - val_loss: 6161231872.0000 - val_mape: 99.9592\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6504649728.0000 - mape: 99.9688 - val_loss: 6160894976.0000 - val_mape: 99.9548\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6504318976.0000 - mape: 99.9659 - val_loss: 6160561152.0000 - val_mape: 99.9501\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 6503980544.0000 - mape: 99.9627 - val_loss: 6160230400.0000 - val_mape: 99.9449\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6503644672.0000 - mape: 99.9595 - val_loss: 6159876096.0000 - val_mape: 99.9395\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6503286784.0000 - mape: 99.9558 - val_loss: 6159512576.0000 - val_mape: 99.9339\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 6502918144.0000 - mape: 99.9522 - val_loss: 6159133184.0000 - val_mape: 99.9282\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 6502407680.0000 - mape: 99.9417 - val_loss: 6158757376.0000 - val_mape: 99.9231\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6501965824.0000 - mape: 99.9346 - val_loss: 6158361600.0000 - val_mape: 99.9176\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6501551104.0000 - mape: 99.9300 - val_loss: 6157952512.0000 - val_mape: 99.9106\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6501090304.0000 - mape: 99.9150 - val_loss: 6157492224.0000 - val_mape: 99.8980\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 6500603392.0000 - mape: 99.9062 - val_loss: 6156943360.0000 - val_mape: 99.8589\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6499983360.0000 - mape: 99.8780 - val_loss: 6156482048.0000 - val_mape: 99.7735\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6499341312.0000 - mape: 99.8572 - val_loss: 6155995648.0000 - val_mape: 99.7353\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 6498842624.0000 - mape: 99.8481 - val_loss: 6155539456.0000 - val_mape: 99.6960\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6498355200.0000 - mape: 99.8392 - val_loss: 6155056640.0000 - val_mape: 99.6558\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6497838080.0000 - mape: 99.8294 - val_loss: 6154546176.0000 - val_mape: 99.5968\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6497308160.0000 - mape: 99.8169 - val_loss: 6154052608.0000 - val_mape: 99.5534\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6496780288.0000 - mape: 99.8004 - val_loss: 6153535488.0000 - val_mape: 99.5275\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 2s 125ms/step - loss: 6496231424.0000 - mape: 99.7856 - val_loss: 6152996352.0000 - val_mape: 99.5039\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 6495663104.0000 - mape: 99.7741 - val_loss: 6152457216.0000 - val_mape: 99.4806\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6495090688.0000 - mape: 99.7638 - val_loss: 6151896064.0000 - val_mape: 99.4566\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 2s 150ms/step - loss: 6494498816.0000 - mape: 99.7536 - val_loss: 6151331328.0000 - val_mape: 99.4327\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6493900288.0000 - mape: 99.7415 - val_loss: 6150763520.0000 - val_mape: 99.4086\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 6493298176.0000 - mape: 99.7309 - val_loss: 6150177280.0000 - val_mape: 99.3839\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6492681216.0000 - mape: 99.7201 - val_loss: 6149591040.0000 - val_mape: 99.3592\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 2s 125ms/step - loss: 6492054528.0000 - mape: 99.7093 - val_loss: 6148975616.0000 - val_mape: 99.3333\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6491406848.0000 - mape: 99.6963 - val_loss: 6148365824.0000 - val_mape: 99.3076\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6490754560.0000 - mape: 99.6867 - val_loss: 6147750912.0000 - val_mape: 99.2818\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6490104320.0000 - mape: 99.6734 - val_loss: 6147119616.0000 - val_mape: 99.2553\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6489435136.0000 - mape: 99.6616 - val_loss: 6146464768.0000 - val_mape: 99.2277\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 6488743936.0000 - mape: 99.6503 - val_loss: 6145799680.0000 - val_mape: 99.1997\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6488038400.0000 - mape: 99.6362 - val_loss: 6145108992.0000 - val_mape: 99.1707\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6487322112.0000 - mape: 99.6233 - val_loss: 6144437760.0000 - val_mape: 99.1424\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 6486604800.0000 - mape: 99.6104 - val_loss: 6143751680.0000 - val_mape: 99.1136\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 6485872128.0000 - mape: 99.5988 - val_loss: 6143042560.0000 - val_mape: 99.0837\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6485126656.0000 - mape: 99.5848 - val_loss: 6142319616.0000 - val_mape: 99.0533\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 6484364288.0000 - mape: 99.5706 - val_loss: 6141584384.0000 - val_mape: 99.0224\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 6483596800.0000 - mape: 99.5566 - val_loss: 6140859904.0000 - val_mape: 98.9919\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6482822656.0000 - mape: 99.5433 - val_loss: 6140128256.0000 - val_mape: 98.9612\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6482038272.0000 - mape: 99.5301 - val_loss: 6139361280.0000 - val_mape: 98.9289\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6481236480.0000 - mape: 99.5148 - val_loss: 6138595840.0000 - val_mape: 98.8966\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 1s 117ms/step - loss: 6480420352.0000 - mape: 99.5015 - val_loss: 6137803776.0000 - val_mape: 98.8633\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6479596544.0000 - mape: 99.4843 - val_loss: 6137031680.0000 - val_mape: 98.8308\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6478767104.0000 - mape: 99.4708 - val_loss: 6136214528.0000 - val_mape: 98.7964\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 6477908992.0000 - mape: 99.4559 - val_loss: 6135417344.0000 - val_mape: 98.7628\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6477062144.0000 - mape: 99.4412 - val_loss: 6134595584.0000 - val_mape: 98.7283\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 6476193280.0000 - mape: 99.4239 - val_loss: 6133774848.0000 - val_mape: 98.6937\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 6475320832.0000 - mape: 99.4083 - val_loss: 6132915200.0000 - val_mape: 98.6575\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 6474423296.0000 - mape: 99.3931 - val_loss: 6132058624.0000 - val_mape: 98.6214\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6473522176.0000 - mape: 99.3777 - val_loss: 6131198976.0000 - val_mape: 98.5852\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6472606720.0000 - mape: 99.3596 - val_loss: 6130316800.0000 - val_mape: 98.5480\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 6471676928.0000 - mape: 99.3451 - val_loss: 6129436672.0000 - val_mape: 98.5109\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6470746624.0000 - mape: 99.3261 - val_loss: 6128567808.0000 - val_mape: 98.4743\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6469812224.0000 - mape: 99.3113 - val_loss: 6127654400.0000 - val_mape: 98.4358\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 6468856320.0000 - mape: 99.2924 - val_loss: 6126720512.0000 - val_mape: 98.3965\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6467873280.0000 - mape: 99.2765 - val_loss: 6125779968.0000 - val_mape: 98.3569\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 6466885120.0000 - mape: 99.2589 - val_loss: 6124849664.0000 - val_mape: 98.3176\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 6465903616.0000 - mape: 99.2397 - val_loss: 6123893248.0000 - val_mape: 98.2773\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6464904192.0000 - mape: 99.2229 - val_loss: 6122954752.0000 - val_mape: 98.2378\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6463894528.0000 - mape: 99.2048 - val_loss: 6121978368.0000 - val_mape: 98.1966\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6462860800.0000 - mape: 99.1850 - val_loss: 6120998400.0000 - val_mape: 98.1553\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 6461832192.0000 - mape: 99.1669 - val_loss: 6120025088.0000 - val_mape: 98.1142\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6460799488.0000 - mape: 99.1482 - val_loss: 6119008768.0000 - val_mape: 98.0713\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 1s 96ms/step - loss: 6459728384.0000 - mape: 99.1279 - val_loss: 6117980672.0000 - val_mape: 98.0280\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 6458642944.0000 - mape: 99.1093 - val_loss: 6116942848.0000 - val_mape: 97.9842\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 1s 107ms/step - loss: 6457556480.0000 - mape: 99.0905 - val_loss: 6115907072.0000 - val_mape: 97.9405\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6456460288.0000 - mape: 99.0715 - val_loss: 6114870272.0000 - val_mape: 97.8968\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6455366656.0000 - mape: 99.0514 - val_loss: 6113846272.0000 - val_mape: 97.8535\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 6454277120.0000 - mape: 99.0312 - val_loss: 6112775168.0000 - val_mape: 97.8084\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6453141504.0000 - mape: 99.0111 - val_loss: 6111676928.0000 - val_mape: 97.7620\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 6451986944.0000 - mape: 98.9897 - val_loss: 6110573568.0000 - val_mape: 97.7154\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 6450829312.0000 - mape: 98.9686 - val_loss: 6109471744.0000 - val_mape: 97.6689\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6449666560.0000 - mape: 98.9486 - val_loss: 6108335104.0000 - val_mape: 97.6209\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6448484864.0000 - mape: 98.9273 - val_loss: 6107253760.0000 - val_mape: 97.5752\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6447322624.0000 - mape: 98.9067 - val_loss: 6106108928.0000 - val_mape: 97.5269\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6446103552.0000 - mape: 98.8846 - val_loss: 6104939008.0000 - val_mape: 97.4774\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6444893184.0000 - mape: 98.8638 - val_loss: 6103780352.0000 - val_mape: 97.4285\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6443668992.0000 - mape: 98.8414 - val_loss: 6102649856.0000 - val_mape: 97.3808\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 6442475520.0000 - mape: 98.8189 - val_loss: 6101472768.0000 - val_mape: 97.3310\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6441232896.0000 - mape: 98.7979 - val_loss: 6100315648.0000 - val_mape: 97.2821\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 6439998976.0000 - mape: 98.7733 - val_loss: 6099154944.0000 - val_mape: 97.2330\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 6438756864.0000 - mape: 98.7532 - val_loss: 6097930752.0000 - val_mape: 97.1813\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 6437481984.0000 - mape: 98.7291 - val_loss: 6096718336.0000 - val_mape: 97.1301\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 6436197376.0000 - mape: 98.7063 - val_loss: 6095474176.0000 - val_mape: 97.0774\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 6434888704.0000 - mape: 98.6848 - val_loss: 6094214656.0000 - val_mape: 97.0242\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6433575424.0000 - mape: 98.6584 - val_loss: 6092983296.0000 - val_mape: 96.9721\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 6432267264.0000 - mape: 98.6346 - val_loss: 6091751424.0000 - val_mape: 96.9200\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6430962688.0000 - mape: 98.6143 - val_loss: 6090492928.0000 - val_mape: 96.8668\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6429633536.0000 - mape: 98.5870 - val_loss: 6089175040.0000 - val_mape: 96.8110\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 6428260352.0000 - mape: 98.5638 - val_loss: 6087925760.0000 - val_mape: 96.7581\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6426927104.0000 - mape: 98.5411 - val_loss: 6086617088.0000 - val_mape: 96.7027\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6425555968.0000 - mape: 98.5138 - val_loss: 6085335552.0000 - val_mape: 96.6485\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 2s 124ms/step - loss: 6424166400.0000 - mape: 98.4901 - val_loss: 6083976192.0000 - val_mape: 96.5909\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 6422757888.0000 - mape: 98.4661 - val_loss: 6082606592.0000 - val_mape: 96.5329\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 6421326336.0000 - mape: 98.4403 - val_loss: 6081283072.0000 - val_mape: 96.4769\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6419930112.0000 - mape: 98.4140 - val_loss: 6079962624.0000 - val_mape: 96.4209\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6418513408.0000 - mape: 98.3884 - val_loss: 6078555136.0000 - val_mape: 96.3613\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6417040384.0000 - mape: 98.3641 - val_loss: 6077146624.0000 - val_mape: 96.3017\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 6415579648.0000 - mape: 98.3363 - val_loss: 6075779584.0000 - val_mape: 96.2437\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6414133760.0000 - mape: 98.3092 - val_loss: 6074428416.0000 - val_mape: 96.1864\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6412691968.0000 - mape: 98.2814 - val_loss: 6073060864.0000 - val_mape: 96.1285\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 6411232256.0000 - mape: 98.2573 - val_loss: 6071585792.0000 - val_mape: 96.0660\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6409705472.0000 - mape: 98.2286 - val_loss: 6070164480.0000 - val_mape: 96.0057\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6408215552.0000 - mape: 98.2012 - val_loss: 6068771840.0000 - val_mape: 95.9466\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6406695424.0000 - mape: 98.1791 - val_loss: 6067277824.0000 - val_mape: 95.8833\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6405156352.0000 - mape: 98.1478 - val_loss: 6065827328.0000 - val_mape: 95.8218\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6403625984.0000 - mape: 98.1188 - val_loss: 6064399360.0000 - val_mape: 95.7611\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 6402102784.0000 - mape: 98.0930 - val_loss: 6062923776.0000 - val_mape: 95.6985\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6400555008.0000 - mape: 98.0643 - val_loss: 6061441536.0000 - val_mape: 95.6356\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6398985216.0000 - mape: 98.0361 - val_loss: 6059893248.0000 - val_mape: 95.5699\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6397365248.0000 - mape: 98.0084 - val_loss: 6058378240.0000 - val_mape: 95.5056\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6395768320.0000 - mape: 97.9791 - val_loss: 6056823296.0000 - val_mape: 95.4396\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6394146304.0000 - mape: 97.9494 - val_loss: 6055345152.0000 - val_mape: 95.3768\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6392554496.0000 - mape: 97.9219 - val_loss: 6053754368.0000 - val_mape: 95.3092\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 2s 134ms/step - loss: 6390905344.0000 - mape: 97.8900 - val_loss: 6052233216.0000 - val_mape: 95.2446\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6389294592.0000 - mape: 97.8602 - val_loss: 6050713600.0000 - val_mape: 95.1800\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 6387662848.0000 - mape: 97.8327 - val_loss: 6049112576.0000 - val_mape: 95.1120\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6385982464.0000 - mape: 97.8032 - val_loss: 6047502848.0000 - val_mape: 95.0436\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6384291328.0000 - mape: 97.7720 - val_loss: 6045906944.0000 - val_mape: 94.9757\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6382619648.0000 - mape: 97.7391 - val_loss: 6044357632.0000 - val_mape: 94.9098\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 6380968960.0000 - mape: 97.7087 - val_loss: 6042738176.0000 - val_mape: 94.8410\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 6379260416.0000 - mape: 97.6793 - val_loss: 6041067008.0000 - val_mape: 94.7699\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6377503232.0000 - mape: 97.6504 - val_loss: 6039407616.0000 - val_mape: 94.6993\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6375778816.0000 - mape: 97.6185 - val_loss: 6037799424.0000 - val_mape: 94.6309\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 2s 141ms/step - loss: 6374064128.0000 - mape: 97.5849 - val_loss: 6036166656.0000 - val_mape: 94.5614\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6372328960.0000 - mape: 97.5563 - val_loss: 6034502656.0000 - val_mape: 94.4905\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6370585600.0000 - mape: 97.5222 - val_loss: 6032770048.0000 - val_mape: 94.4168\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6368783360.0000 - mape: 97.4879 - val_loss: 6031115264.0000 - val_mape: 94.3463\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6367031296.0000 - mape: 97.4557 - val_loss: 6029469184.0000 - val_mape: 94.2762\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6365264896.0000 - mape: 97.4261 - val_loss: 6027737088.0000 - val_mape: 94.2024\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6363429888.0000 - mape: 97.3957 - val_loss: 6026048512.0000 - val_mape: 94.1305\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6361663488.0000 - mape: 97.3621 - val_loss: 6024319488.0000 - val_mape: 94.0568\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6359835648.0000 - mape: 97.3268 - val_loss: 6022560256.0000 - val_mape: 93.9818\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6357975040.0000 - mape: 97.2958 - val_loss: 6020725760.0000 - val_mape: 93.9036\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6356091392.0000 - mape: 97.2630 - val_loss: 6018994176.0000 - val_mape: 93.8298\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6354273792.0000 - mape: 97.2287 - val_loss: 6017351168.0000 - val_mape: 93.7597\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6352474624.0000 - mape: 97.1984 - val_loss: 6015517696.0000 - val_mape: 93.6815\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6350583296.0000 - mape: 97.1595 - val_loss: 6013742080.0000 - val_mape: 93.6058\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 6348686336.0000 - mape: 97.1284 - val_loss: 6011880448.0000 - val_mape: 93.5263\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6346758144.0000 - mape: 97.0912 - val_loss: 6010066944.0000 - val_mape: 93.4489\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 6344843776.0000 - mape: 97.0563 - val_loss: 6008284160.0000 - val_mape: 93.3728\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6342960128.0000 - mape: 97.0206 - val_loss: 6006459904.0000 - val_mape: 93.2949\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 6341023232.0000 - mape: 96.9864 - val_loss: 6004575744.0000 - val_mape: 93.2145\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 6339052544.0000 - mape: 96.9504 - val_loss: 6002738176.0000 - val_mape: 93.1360\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6337113600.0000 - mape: 96.9151 - val_loss: 6000917504.0000 - val_mape: 93.0582\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 6335173632.0000 - mape: 96.8808 - val_loss: 5998990336.0000 - val_mape: 92.9758\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 1s 124ms/step - loss: 6333147136.0000 - mape: 96.8432 - val_loss: 5997108736.0000 - val_mape: 92.8954\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 6331169280.0000 - mape: 96.8103 - val_loss: 5995202048.0000 - val_mape: 92.8139\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 6329155072.0000 - mape: 96.7735 - val_loss: 5993335296.0000 - val_mape: 92.7341\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 2s 146ms/step - loss: 6327190528.0000 - mape: 96.7351 - val_loss: 5991405056.0000 - val_mape: 92.6515\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 6325153792.0000 - mape: 96.6988 - val_loss: 5989504512.0000 - val_mape: 92.5702\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 2s 148ms/step - loss: 6323144704.0000 - mape: 96.6628 - val_loss: 5987554304.0000 - val_mape: 92.4868\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 6321076736.0000 - mape: 96.6277 - val_loss: 5985626624.0000 - val_mape: 92.4043\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 2s 155ms/step - loss: 6319032832.0000 - mape: 96.5909 - val_loss: 5983627776.0000 - val_mape: 92.3187\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6316977152.0000 - mape: 96.5624 - val_loss: 5981710336.0000 - val_mape: 92.2366\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 6314912768.0000 - mape: 96.5360 - val_loss: 5979668480.0000 - val_mape: 92.1492\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 6312787968.0000 - mape: 96.5087 - val_loss: 5977705472.0000 - val_mape: 92.0651\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 6310737920.0000 - mape: 96.4864 - val_loss: 5975773184.0000 - val_mape: 91.9823\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 6308651008.0000 - mape: 96.4617 - val_loss: 5973754368.0000 - val_mape: 91.8958\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 6306545152.0000 - mape: 96.4315 - val_loss: 5971729408.0000 - val_mape: 91.8090\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 6304386048.0000 - mape: 96.4072 - val_loss: 5969629184.0000 - val_mape: 91.7190\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6302209024.0000 - mape: 96.3817 - val_loss: 5967577088.0000 - val_mape: 91.6309\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6300062208.0000 - mape: 96.3525 - val_loss: 5965621248.0000 - val_mape: 91.5471\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6297961472.0000 - mape: 96.3282 - val_loss: 5963559424.0000 - val_mape: 91.4586\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6295784448.0000 - mape: 96.3028 - val_loss: 5961435648.0000 - val_mape: 91.3675\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6293570560.0000 - mape: 96.2726 - val_loss: 5959380480.0000 - val_mape: 91.2793\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 2s 168ms/step - loss: 6291407360.0000 - mape: 96.2494 - val_loss: 5957314048.0000 - val_mape: 91.1906\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6289189888.0000 - mape: 96.2204 - val_loss: 5955155456.0000 - val_mape: 91.0979\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 3s 212ms/step - loss: 6286933504.0000 - mape: 96.1907 - val_loss: 5953028096.0000 - val_mape: 91.0065\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6284728320.0000 - mape: 96.1665 - val_loss: 5950963200.0000 - val_mape: 90.9178\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 2s 164ms/step - loss: 6282517504.0000 - mape: 96.1411 - val_loss: 5948808192.0000 - val_mape: 90.8252\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 3s 213ms/step - loss: 6280272896.0000 - mape: 96.1091 - val_loss: 5946719232.0000 - val_mape: 90.7354\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6278035456.0000 - mape: 96.0821 - val_loss: 5944541184.0000 - val_mape: 90.6417\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 3s 235ms/step - loss: 6275776000.0000 - mape: 96.0554 - val_loss: 5942462976.0000 - val_mape: 90.5524\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6273534976.0000 - mape: 96.0270 - val_loss: 5940274176.0000 - val_mape: 90.4583\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6271261696.0000 - mape: 96.0005 - val_loss: 5938202112.0000 - val_mape: 90.3691\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 2s 159ms/step - loss: 6269037568.0000 - mape: 95.9724 - val_loss: 5935952896.0000 - val_mape: 90.2723\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6266678784.0000 - mape: 95.9416 - val_loss: 5933764096.0000 - val_mape: 90.1781\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6264390144.0000 - mape: 95.9147 - val_loss: 5931584000.0000 - val_mape: 90.0842\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6262073344.0000 - mape: 95.8906 - val_loss: 5929377792.0000 - val_mape: 89.9892\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6259736576.0000 - mape: 95.8627 - val_loss: 5927077888.0000 - val_mape: 89.8901\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 2s 152ms/step - loss: 6257354752.0000 - mape: 95.8250 - val_loss: 5924867072.0000 - val_mape: 89.7949\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 2s 157ms/step - loss: 6255002624.0000 - mape: 95.7998 - val_loss: 5922608128.0000 - val_mape: 89.6975\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6252627456.0000 - mape: 95.7692 - val_loss: 5920367616.0000 - val_mape: 89.6009\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 2s 149ms/step - loss: 6250272256.0000 - mape: 95.7414 - val_loss: 5918111232.0000 - val_mape: 89.5036\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 2s 156ms/step - loss: 6247875072.0000 - mape: 95.7133 - val_loss: 5915814912.0000 - val_mape: 89.4046\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6245458944.0000 - mape: 95.6858 - val_loss: 5913516544.0000 - val_mape: 89.3054\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 2s 158ms/step - loss: 6243068416.0000 - mape: 95.6541 - val_loss: 5911335936.0000 - val_mape: 89.2113\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqTUlEQVR4nO3dd3hc1Z3/8fdRs5plWc2SVSz3gnDHmGJCMcYNGwwYCB2Cw4ZkYVNYAr9NstlsEkJ62ISFhWCKKYlpBmNsjDHF3cZF7lUualbvdc7vjzsGY9ylmTsafV7Po2dGVzNzv9wZfzhz7rnnGGstIiISXELcLkBERNqfwl1EJAgp3EVEgpDCXUQkCCncRUSCUJjbBQAkJSXZ7Oxst8sQEelQ1q5dW2KtTT7e3wIi3LOzs1mzZo3bZYiIdCjGmLwT/U3dMiIiQUjhLiIShBTuIiJBSOEuIhKEFO4iIkHolOFujHnWGFNsjMk9aluCMWaRMWan97a7d7sxxvzZGLPLGLPRGDPSl8WLiMjxnU7L/Tlg4jHbHgYWW2v7A4u9vwNMAvp7f2YBf2ufMkVE5Eyccpy7tfZjY0z2MZunA5d6788GPgL+3bv9eevMI7zCGBNvjEmz1ha0W8VH2bz2Ywo3f0xIaAQhoaGY0AhCwsIhPIqQiGiIiCGkSyxhkbGERcYQHhVHl6gYuoSHERkeSmR4CF3CQgkPNRhjfFGiiIgrzvYiph5HBXYh0MN7Px04cNTjDnq3fS3cjTGzcFr3ZGVlnVUR1ZsXccWeP5/RczzWUE8EdURSaiOpI5I6utBgImk0UTSERFMXGkt9aBwNYXE0hnWlKaIbnoh4orol0DU+hW4JiXSNjqZHXCS9EqPpGhl+VvWLiPhKm69QtdZaY8wZr/hhrX0KeApg9OjRZ7ViyNibH8E2fo+W5iaam5tpaW6kuamJlqY6WhtqaW2swdNYQ2tDHa2N1djGWmiqxTbXYZpqCWmuJbKljpiWOsJa6ghrPUxEaw1RLdVENtd/fYcHv7xbYyMpsIkstRm8Ez6RlqxxTBmWxsX9kknu2uVs/nNERNrN2YZ70ZHuFmNMGlDs3X4IyDzqcRnebb4RHoUJjyIcaPe2c2szNFRCfTnUV0BDBdSX01BdSl1lCa21ZcSV7+fyw+uY2rySj/afz73b7qeZMFLjIknvHkV6fNRXbrMTY+iVEE1IiLqARMS3zjbc3wbuAH7tvX3rqO3fNca8ApwPVPqqv93nQsMhJsn5OUqk9+cLzQ2w4n+4dPHPWT0wnjcyHyG3zHCooo7PD5Qzf1MBLZ4vv5jERIQyKC2Ogald6Z0YQ6/EaLKTYshKiCYyPNQv/2kiEvxOGe7GmJdxTp4mGWMOAj/FCfXXjDH3AHnATO/D5wOTgV1AHXCXD2oOLOGRMO4HEB5D/IJ/567C5TD6bpj0Hejag1aPpbi6gfyKenYX17KloIot+VXM31RARV3zFy9jDKTFRdIrMYbspGinle+93yshhqgIBb+InD4TCAtkjx492gbFrJAFG+DTP8KWNyEkHEbcAhc9AN2zj/vwirom8krr2Fday76SOvJKa9lXWkteaR2ltU1feWyq9+RtdmIMvbzhn+1t+cd0CYjJPUXEz4wxa621o4/7N4W7D5TuhmV/hvVzICQMrvoljLrTaZ6fpsr6ZvZ7gz+vtJa9X4R/HSU1jV95bHLXLmR7gz87KebL/wloJI9IUFO4u6XyELx1P+xZAumj4OLvQ9/LISK6TS9b09jiBH3Jl+G/r7SOfSW1FFd/PfgH9ujKoNSuDEztyqDUOPr3iFX/vkgQULi7yeOBDXPgo19D5QEIi4TscdB/AvS/EhJ6t+vu6ppayCv9spW/q7iG7YXV7CiqprHFA0CIgezEGAZ+EfhO6GdpJI9Ih6JwDwQtTZD3KexYCDsXQtluZ3vSQBgwAQZMhMzznVE6PtDqseSV1rK9sJpthdXe2yryyuo48hGICg9lQI9Yb+jHfdHaT4rVuH2RQKRwD0Slu2HH+7Dzfdj3GXiaIbIb9BvvBH2/8RCd4PMy6ppa2FlU82XoF1WxvbCakpovT+gmxUYwKDWOc3rGMaRnHDnp3eidGKNWvojLFO6BrrEadi/5MuxrD4MJgYwxMOAqJ+xTBp/RCdm2KqlpZHthNVsLqr7S2m9qdbp2YiJCGZzmBP2QnnHk9OxG/x6xhIdqFmkRf1G4dyQeDxR87gT9jgXO8EqAblkweCoMngaZYyDE/ydEm1s97CquIfdQJZvzq9ic79zWNbUCEBEawuCecQzP6MbwrHiGZcSTrRa+iM8o3DuyqgKnj377fNj9IbQ2QWwPGDTFCfrsi33WT386PB7L3tJaNudXkXuokg0HKth0qPKLwI+LDGNYphP0wzPjGdmrOwkxEa7VKxJMFO7BoqHKCfqtb8PORdBcB1HdYcg1MPRG54RsiPvdIq0ey67iGjYcqODzAxVsOFDB9qJqWr3TMPRJimFkr+6M8v70S45V617kLCjcg1FTHexeDJvfhG3vQku903Uz9AY4dyakDHK7wq+ob2pl06FK1u0vZ21eOevyyr+4CrdrZBgjs74M+2GZ8cTqqluRU1K4B7vGGifgN77qXDBlPZA6FIbOhJzrIS7N7Qq/xlpLXmkda/PKWbvfCfvtRdVY64zDH5wWx5jeCZzfO4HR2QkajilyHAr3zqSmGHJfd4I+fx1goPclTrfN4KshMs7tCk+oqqGZ9fsrWJNXztq8MtbmldPQ7IzO6Zscw5jeiZzfO4HzeieQHh/lcrUi7lO4d1Ylu2DTa7DxNSjf61wdO3ASDL/FmQbBhRE3Z6KpxUNufiWr9paxam8Zq/eVUd3QAkBWQjQX9k3kAu9PStfIU7yaSPBRuHd21sLBNU7Q586FulLolgkjboMRt0K3dLcrPC2tHsu2wipW7ilj+Z5SVuwp/SLs+6fEesM+ibF9EoiP1ogcCX4Kd/lSS5MzrHLtc07/vAmBflc6s1b2nwChHedEZqvHsjm/kuW7S1m2u5RVe8uob27FGDinZxwX9k3igr6JnJedoBO0EpQU7nJ8ZXvh8xfg85egphC6pjldNiNvh+693K7ujDW1eNh4sIJlu0tZtruEdXkVNLV6CAsxDMuM/6IbZ2RWd82KKUFB4S4n19riTHuw9jln/DxAvytgzLedOW4CYOz82WhobmVtXjnLdpewbHcpGw9W0uqxRISFMCqrO5cOTOayQSn0T4nF+HFqB5H2onCX01dxwGnNr30OaoogoQ+cd6+zqlRkN7era5PqhmZW7ytj2a5SPt1VwrbCagDS46O4bFAylw9K4YI+SVrSUDoMhbucuZYm50rYlf8LB1dBeAwMuwnGzAq4C6TOVkFlPR9tP8yH24r5bFcJdU2tdAkL4YK+iVw+KIXLBqaQmdC2hVVEfEnhLm2T/zmseho2/RNaG6H3N+CC7zqLjQRJd0ZjSyur9pbx4bZilmwrZl9pHQD9UmK5fFAKlw5M5rzsBM16KQFF4S7to7YE1s2G1c9A1SFIHgwXfg/OvR7CgusK0r0ltXy4rZiPthezck8ZTa0eunYJ4+L+SVzmDXuNrRe3KdylfbU2O1fBLvszFOVCbCqMvQ9G3QVR8W5X1+5qG1v4bFcJS7YXs2TbYQqrGgA4N70blw1K4bKByQzLiNfkZ+J3CnfxDWudaYiX/Rn2fAQRsc54+fPvg/hMt6vzCWstWwuqvUFfzLr95XgsJMZE8I0BzuibS/on0y3avWmYpfNQuIvvFWyEZX9xroA1Bs6ZARc9AKk5blfmU+W1TXy88zBLthXz0Y7DVNQ1ExpiGJXVnSsGpzApJ42sRJ2UFd9QuIv/VByAlU86QymbamDgFLjkB5A+yu3KfK7VY1l/oIIl24r5cFsxWwqqAOdq2Uk5qUzMSaNfSqzLVUowUbiL/9WXw8qnYMVfoaHCmahs3A8h+yK3K/ObA2V1LMgt5L3cAtbtrwBgQI9YJuakMfncVAb26KqLp6RNFO7insZqZ3TN8iechb+zLnRa8n2vCJphlKejoLKe93MLeS+3kNX7yvBY6J0Uw4QhPbi4fxLn904kIkzDLOXMKNzFfU11zpWvn/3JGUbZcyRc+uOgGit/ug5XN7JwSyELcgtZsaeU5lZLfHQ4U85NY8bIdEZmdVeLXk6Lz8LdGPMAcC9ggKettX80xgwHngQigRbgO9baVSd7HYV7J9LSCBtehk9+BxX7IX00XPaI023TCQOtrqmF5btLeWt9Pgu3FNLQ7CE9PopJOalMHprGcA2xlJPwSbgbY3KAV4AxQBOwALgP+CvwB2vte8aYycBD1tpLT/ZaCvdOqKUJNsyBpY9D1UHIHAuXP+qsGtVJ1TS28H5uIfM3FfDJzhKaWj2kdYtkUk4aU4elMSIzXi16+YqThXtbJrkeDKy01tZ5d7IUmAFY4Mhabt2A/DbsQ4JVWIQzJn7YzU53zce/g9lXO33x438KacPcrtDvYruEcd2oDK4blUFVQzMfbCli/qZCXlyRx7Of7SWjexRThqZx9dCenNMzTkEvJ9WWlvtg4C3gAqAeWAyswWm5v4/TVRMCXGitzTvO82cBswCysrJG5eV97SHSmTTXO/PXfPI7Z3RNzvVOSz6hj9uVua6qoZlFm4uYtzGfT3eW0OKx9E6K4eqhaVwzIp0+yRpe2Vn5ss/9HuA7QC2wGWjECfSl1tq5xpiZwCxr7fiTvY66ZeQL9RXOSdcVfwNPszOlwTcegtgUtysLCOW1TSzYXMg7G/NZvrsUj4URWfFcNzKDqUPTtLxgJ+OX0TLGmF8CB4FfAfHWWmuc742V1tq4kz1X4S5fU1UASx+Ddc87C3tfcL8zSVnkST9KnUpRVQNvfn6IuesOsqOohvBQwzcGJDNteDrjB6cQHaGlBYOdL1vuKdbaYmNMFrAQGAssB/7FWvuRMeYK4DfW2pNenqhwlxMq2QVLfgGb34DoROdCqPPuCbpZKNvCWsvm/Cre3pDP2+vzKaxqIDoilKvOSWXGyHQu7JtEqEbcBCVfhvsnQCLQDHzfWrvYGHMx8Ceck7UNOEMh157sdRTuckqH1sHi/3QmKOuWBVf8xJlqWCcVv8LjsazaV8Zb6w/xzsYCqhtaSOsWyTUj0rluZIamPwgyuohJgsfuJfDBT6FgA2ScBxN/DRnH/Wx3eg3NrXywtYi5aw/y8c4SWj2WYRnduHZEOlcP60lirL79dHQKdwkuHo9zIdTi/3TWeT13pjN8sluG25UFrOLqBt5en8/r6w6xpaCKsBDD5YNSuGlMJpf0TyZMK0x1SAp3CU6N1fDpH52phk2IM8XwRf8KETFuVxbQthdW8/q6g8xdd5CSmiZS4yK5flQGM0dnanriDkbhLsGtPA8++Blsfh269oTxP4OhM9UffwrNrR4Wby3m1dX7WbrjMB4LF/VL5MbzspgwpAeR4aFulyinoHCXziFvOSx4GArWO9MZTH4c0oa6XVWHkF9Rzz/XHuTV1Qc4VFFPfHQ404f15PpRmeSk62rYQKVwl87D44H1LzknXevLYfQ9zsRk0QluV9YheDyWz3aX8OrqAyzcUkRTi4cBPWK5flQG14xI16LgAUbhLp1PfTks+RWsfhqiusMVP4URt0GIThyersr6Zt7ZmM/ctQdZt7+CsBDDFYNTuHlMFuP6J2vsfABQuEvnVbgJ5v8I9i935pCf/FvICP4l/9rb7sM1vLb6AP9Ye5Cy2ibS46O46bxMZp6XSY84tebdonCXzs1a2PQPWPj/oKYYRt7mtORjktyurMNpavGwcEshL6/az2e7SgkNMYwfnMI3z+/FuH5JmnvezxTuIgANVc58NSufdIZLXv4fzsRkoZqD5WzsLanllVX7v2jNZ3SP4uYxWdwwOkN9836icBc5WvE2eO8h2LsUUofCtD9DzxFuV9VhNba0snBzEXNW7mf5nlLCQgwTc1K566JsLRnoYwp3kWNZC1vehPf+3Vm4+4L74dJHIEIX8bTFnsM1zFm5n1fXHKC6oYVz07tx54XZTB2WRpcwjZtvbwp3kROpr4BFP4F1s6F7Nlz9J+hzqctFdXy1jS288fkhnlu2j13FNSTFRnDTeVl88/wsesZHuV1e0FC4i5zK3k9g3gNQthuG3wITfqGx8e3AWsunu0qYvWwfi7cVE2IMVw7uwe0X9uKCPonqsmkjhbvI6Wiuh48fd1aCiuoOkx6Dc2ZoGoN2cqCsjhdX5vHq6gNU1DXTPyWW2y7oxYyRGcR20Unts6FwFzkThZvg7e9B/ucwYCJM+Z1mnGxHDc2tzNuQz/PL89h0qJLYLmHMGJnO7Rf0ol9KV7fL61AU7iJnytPqDJn88BfOjJPjf+ZMZaArXNuNtZb1Byp4YXke72wsoKnVw7j+SdxzcW8u6Z+sMfOnQeEucrbK98E7/wa7P4SMMc6wyZTBblcVdEpqGnll1X6eX55HcXUjfZNjuOui3swYma61YE9C4S7SFtbCxtecGScbq52JyC78V1385ANNLR7mbyrgmU/3sulQJd2iwrl5TBZ3XNiLtG4aZXMshbtIe6gtgXe/D1vegvTRcO2TkNTf7aqCkrWWtXnlPPvZXhbkFmKMYfK5adx9UTbDM+M1ysZL4S7SXqyF3Lkw/4fO6JrxP4Mx31ZfvA8dKKvj+eX7eGW1c2FU3+QYJp+bxqScNAande3UQa9wF2lv1YXOuPgdCyB7HFzzV4jPcruqoFbT2MKbnx9i/qYCVuwpxWMhOzGaq3JSuWxgCqN6dSe8k60Fq3AX8QVrnYVB3nvYGQs/6Tcw7CaNi/eD0ppGFm4pYv6mApbvLqXFY4ntEsZF/RKZmJPKFYN7EBcZ7naZPqdwF/Gl8jx44z7YvwwGXw1T/wQxiW5X1WlUNzSzfHcpS3ccZvHWYgqrGggPNVzUL4mrzkll/OAeJHft4naZPqFwF/E1Tyssf8IZFx8ZD9OfgAFXuV1Vp+PxWNYfrOC9TQW8l1vIwfJ6jIFRWd2ZcE4PJgxJJTspxu0y243CXcRfCnPhjW9DUa4zV/xV/+3MHS9+Z61lW2E1CzcXsXBLIZvzqwAY0COWacN6Mm1YOlmJHXsWUIW7iD+1NDot+GV/gcS+cN3/ab74AHCgrI4Ptjr99Kv3lQMwKLUrE85JZcKQHpzTM67DjbxRuIu4Ye/HTl98TRFc9ihc9ACEaE7zQHCgrI73NxeycEsRa/aV4bGQlRDNxJxUrjqnB8Mzu3eIBcB9Fu7GmAeAewEDPG2t/aN3+/eA+4FW4F1r7UMnex2FuwSt+nKY96CzMEivi50Ln+Iz3a5KjlJa08gHW4t4L7eQz3aV0NxqSYqNYGJOKtcMT2dkVveAnefGJ+FujMkBXgHGAE3AAuA+IBN4FJhirW00xqRYa4tP9loKdwlq1sKGl2H+j8CEwtV/gJzr3K5KjqOyvpmlOw7z/uZCFm8toqHZQ3p8FFOHpjHp3DSGZXQLqK4bX4X7DcBEa+093t//A2gERgNPWWs/ON3XUrhLp1C2B17/NhxcBUNvgsmPQ2Sc21XJCdQ0trBwcyFvrc/ns10ltHgs6fFRTMpJZdK5aYzIjHe9Re+rcB8MvAVcANQDi4E1wDjv9olAA/BDa+3qk72Wwl06jdYW+OS3sPQ30C0dZjwNWWPdrkpOobKumUVbi3hvUwGf7CyhqdVDalwkk89NY9rwnq616H3Z534P8B2gFtiM03IfDywB/hU4D3gV6GOP2ZExZhYwCyArK2tUXl7eWdch0uEcWAWv3wsVB2D8T51ZJgPo676cWFVDM4u3FvHuxkI+3nGYplYPmQlRTD43jQlDUv3aovfLaBljzC+Bg8A04DFr7RLv9t3AWGvt4RM9Vy136ZQaquDt7zqzTA6c4sxPExXvdlVyBirrm1m4uZC3N+R/MQ1CUmwXpg/vyZ0XZpOZ4Ntx9L5suadYa4uNMVnAQmAscBPQ01r7E2PMAJzumqxjW+5HU7hLp2UtrPxfWPios5TfDbOh53C3q5KzUFnfzEfbi50hlpuL8FjLmN4JTBnak4nnpPpkCgRfhvsnQCLQDHzfWrvYGBMBPAsMxxlF80Nr7Ycnex2Fu3R6B1bBP+505oyf/BsYeYe6aTqwgsp6Xl51gHc35rP7cC0hBi7om8jVQ3syMSeV+OiIdtmPLmIS6QhqS+H1bzlL+g27Gab8HiI69uXxnZ21lu1F1by7sYB5G/LZV1pHeKjhkv7JXD2sJ1cO6UFMl7Nf0UvhLtJReFrh49/CR79y1mqd+bxWewoS1lpyD1Uxb2M+8zbkU1DZQGR4CD+fnsPM0Wd3YZvCXaSj2f0hzP2WM0/NtL9Azgy3K5J25PFY1u4vZ96GfK4flcHQjPizeh2Fu0hHVHnI6Yc/uArOvw+u/C8Ia5++WgkOJwv3zrUmlUhH0i0d7poPY++HlU/C3yc54+JFToPCXSSQhYbDxF86fe+Ht8P/XgK7TntmD+nEFO4iHcGQ6TDrI4jrCS9eD0t+6Zx8FTkBhbtIR5HUD+5ZBMO/CUsfgxdnOOPiRY5D4S7SkUREO9MUTHsC9q+AJ8c5tyLHULiLdEQjb3Na8WFd4LkpsPx/nKkMRLwU7iIdVdpQ+PZSGDAR3n/EGRffXO92VRIgFO4iHVlkN7jxRbjiJ5A71xkuWZXvdlUSABTuIh2dMTDuB3DTHCjZCU9dBgfXul2VuEzhLhIsBk329sNHOC34ja+5XZG4SOEuEkx6DIF7P4KM85yVnhb9VOPhOymFu0iwiUmE296AUXfBZ3+EV77prPoknYrCXSQYhUXA1X+Eyb+FnYvgmSuhbI/bVYkfKdxFgtmYe51WfHUhPH057FnqdkXiJwp3kWDX5xswawnEpMAL18Kqp92uSPxA4S7SGST0gW99AP3Gw/wfwjv/Bq3NblclPqRwF+ksIuPg5pfhogdgzbPw/DXOuq0SlBTuIp1JSChc+XO49ik4uBqevsyZJ16CjsJdpDMadqOzylNznTOSZu/Hblck7UzhLtJZZYyGby2GrmnOidbPX3K7ImlHCneRzqx7L7j7feh1Ebz1HfjwF5o6OEgo3EU6u6h4uHUujLgVPn7cmbagucHtqqSNwtwuQEQCQGi4s7pTQh9Y/HOoPAg3vuRMZSAdklruIuI4MnXw9c/CoXXwzHgo3e12VXKWFO4i8lU518Edb0N9BfzfFZC3zO2K5Cy0KdyNMQ8YY3KNMZuNMQ8e87cfGGOsMSapTRWKiP9ljXWuaI1OhOenQ+7rblckZ+isw90YkwPcC4wBhgFTjTH9vH/LBCYA+9ujSBFxQWJfZ/GP9FHwz7thxd/crkjOQFta7oOBldbaOmttC7AUmOH92x+AhwCNqRLpyKITnFklB02BBQ/Dop+Ax+N2VXIa2hLuucA4Y0yiMSYamAxkGmOmA4estRtO9mRjzCxjzBpjzJrDhw+3oQwR8anwKJj5PIy+Gz77E7x5H7Q0uV2VnMJZD4W01m41xjwGLARqgfVAF+ARnC6ZUz3/KeApgNGjR6uFLxLIQkJhyu+ha09Y8guoPewEfpeublcmJ9CmE6rW2mestaOstZcA5cBmoDewwRizD8gA1hljUttcqYi4yxj4xo9g2l+cRT+emwo1xW5XJSfQ1tEyKd7bLJz+9tnW2hRrbba1Nhs4CIy01ha2uVIRCQwjb3emDj683Zl0TGPhA1Jbx7nPNcZsAeYB91trK9pekogEvAFXwR3znIW3n5ngXPQkAaWt3TLjrLVDrLXDrLWLj/P3bGttSVv2ISIBKvM8uGchREQ7XTS7vhYB4iJdoSoiZy+pvzMWPqEPzLkRNr/hdkXipXAXkbbpmgp3vuNc7PSPu2DN392uSFC4i0h7iIp3LnbqNx7eeRA++b3mhXeZwl1E2kdEtDOK5twbYPF/wqL/UMC7SPO5i0j7CQ13Ft+OjIdlf4H6cpj6JwhV1PibjriItK+QEJj8uDMvzdLHoKESZvwfhEe6XVmnom4ZEWl/xsBlj8DEX8PWeTBnJjTWuF1Vp6JwFxHfGfsvcM2TsO8TeOl656In8QuFu4j41vCbnaX7Dq6GF65x+uHF5xTuIuJ751zrzCJZsBFmT4O6MrcrCnoKdxHxj0FTvpxwTDNK+pzCXUT8p/+VcMtrULYHnpsCVQVuVxS0FO4i4l99LoVb50JVPvx9ElQccLuioKRwFxH/y74IbnvT6Xt/bjKU73O7oqCjcBcRd2SeB3e85QyP/PtkLfrRzhTuIuKeniOcGSVbGpwumuJtblcUNBTuIuKu1HPhzvnO/eemQGGuu/UECYW7iLgvZZAT8KERMHsq5H/udkUdnsJdRAJDUj+4az5EdIXZ0+HAarcr6tAU7iISOBJ6OwEfneBMVZC3zO2KOiyFu4gElvhMuOs9iOsJL14Hez5yu6IOSeEuIoEnLg3ufBe6Z8NLM2HnIrcr6nAU7iISmGJT4I53IHkAvPJN2Dbf7Yo6FIW7iASumES4Y54zXPK122H7Arcr6jAU7iIS2KK6w62vQ49z4LXbYNdityvqEBTuIhL4ouLhtjcgydtFs/cTtysKeAp3EekYohPg9reck6xzboT9K9yuKKAp3EWk44hJcgK+ayq8eD0cXOt2RQGrTeFujHnAGJNrjNlsjHnQu+1xY8w2Y8xGY8wbxpj49ihURARwgv2OeU5L/sVrIX+92xUFpLMOd2NMDnAvMAYYBkw1xvQDFgE51tqhwA7gx+1RqIjIF7qlOwHfJQ5euBaKNrtdUcBpS8t9MLDSWltnrW0BlgIzrLULvb8DrAAy2lqkiMjXdO/ldNGEdYHnp8PhHW5XFFDaEu65wDhjTKIxJhqYDGQe85i7gfeO92RjzCxjzBpjzJrDhw+3oQwR6bQS+8LtbwMGZl+tBT+Octbhbq3dCjwGLAQWAOuB1iN/N8Y8CrQAL53g+U9Za0dba0cnJyefbRki0tklD3Ba8K1NMHsalOe5XVFAaNMJVWvtM9baUdbaS4BynD52jDF3AlOBW6y1ts1VioicTI8hcPub0FTttOArD7pdkevaOlomxXubBcwA5hhjJgIPAdOstXVtL1FE5DSkDXMudKorc1rw1YVuV+Sqto5zn2uM2QLMA+631lYATwBdgUXGmPXGmCfbuA8RkdOTPgpunesE++xpUNN5z+eFteXJ1tpxx9nWry2vKSLSJlnnwy2vORc5PT/dWYA7OsHtqvxOV6iKSPDJvhhungOlu5wVneor3K7I7xTuIhKc+l4ON74ARVucFZ0aqtyuyK8U7iISvAZcBTf8HfI/hzkzoanW7Yr8RuEuIsFt8NVw3dNwYKUzXXBLo9sV+YXCXUSCX851MO0JZ7HtN74NntZTPqWja9NoGRGRDmPELVBXAot+AtFJMPlxMMbtqnxG4S4incdFD0BNMSx/wlmA+xsPuV2RzyjcRaRzufK/oLYElvy3s/jH6LvdrsgnFO4i0rmEhMD0J6C+DN79gdNFM2Sa21W1O51QFZHOJzQcbnjOma5g7j1BueC2wl1EOqeIGPjma9C9N7x8MxRscLuidqVwF5HOKzoBbnsdIrs5c9GU7XG7onajcBeRzq1bhhPwnmZnPdbqIrcrahcKdxGR5IFwyz+dYZIvXQcNlW5X1GYKdxERgIzRMPMFKN4Kr9wCzQ1uV9QmCncRkSP6j4dr/gb7PoHXv9WhpylQuIuIHG3oTLjqV7B1njMOvoMuA62LmEREjnXBd6C2GD79gzNNwWWPuF3RGVO4i4gczxU/hdrDsPQx5yrW82e5XdEZUbiLiByPMTD1T1BXBu895MxDkzPD7apOm/rcRUROJDQMrn8WssbC67Ng9xK3KzptCncRkZMJj4KbX4akAfDqrXBondsVnRaFu4jIqUR1h1vnOtMVvHQDlO52u6JTUriLiJyOuDS49Q3AwgvXQFWB2xWdlMJdROR0JfVzpimoK4MXr4P6CrcrOiGFu4jImUgfCTe+CCU74OWboLne7YqOS+EuInKm+l4GM56C/Svgn3dDa4vbFX1Nm8LdGPOAMSbXGLPZGPOgd1uCMWaRMWan97Z7u1QqIhJIcmbA5Mdh+3x454GAm6bgrMPdGJMD3AuMAYYBU40x/YCHgcXW2v7AYu/vIiLBZ8y9cMlD8PmL8OEv3K7mK9rSch8MrLTW1llrW4ClwAxgOjDb+5jZwDVtqlBEJJBd9giMvAM++S2se8Htar7QlnDPBcYZYxKNMdHAZCAT6GGtPTJGqBDocbwnG2NmGWPWGGPWHD58uA1liIi4yBiY8jvoezm882DAXMV61uFurd0KPAYsBBYA64HWYx5jgeN2RFlrn7LWjrbWjk5OTj7bMkRE3BcaDjfMhqSB8NrtULTF7YradkLVWvuMtXaUtfYSoBzYARQZY9IAvLfFbS9TRCTARcbBLa9BeDTMmQnVha6W09bRMine2yyc/vY5wNvAHd6H3AG81ZZ9iIh0GN0y4JuvOhc5zbkRmmpdK6Wt49znGmO2APOA+621FcCvgSuNMTuB8d7fRUQ6h57DnZkkCzfCXPeW6mvTfO7W2nHH2VYKXNGW1xUR6dAGToSJj8F7P4L3H4FJj/m9BC3WISLiC+fPgvK9sOKv0L03jL3Pr7tXuIuI+MqEX0B5Hix4GLr3goGT/LZrzS0jIuIrIaFw3dNOP/w/74b89f7btd/2JCLSGUXEwM2vQFSCM4tk5SG/7FbhLiLia11TnTHwjTXOEMnGap/vUuEuIuIPPc6Bmc9B8Ra/TBOscBcR8Zd+451pgncudE6y+nCaYI2WERHxp/PugbI9sPwJSOwLY//FJ7tRuIuI+NuVP4fyfbDgxxDfCwZNbvddqFtGRMTfQkKdZfr6T4DYFJ/sQi13ERE3RMQ4I2h8RC13EZEgpHAXEQlCCncRkSCkcBcRCUIKdxGRIKRwFxEJQgp3EZEgpHAXEQlCxvpw4prTLsKYw0DeWT49CShpx3LaU6DWprrOjOo6c4FaW7DV1ctam3y8PwREuLeFMWaNtXa023UcT6DWprrOjOo6c4FaW2eqS90yIiJBSOEuIhKEgiHcn3K7gJMI1NpU15lRXWcuUGvrNHV1+D53ERH5umBouYuIyDEU7iIiQahDh7sxZqIxZrsxZpcx5mEX68g0xiwxxmwxxmw2xjzg3f4zY8whY8x670/7r6V16tr2GWM2efe/xrstwRizyBiz03vb3c81DTzqmKw3xlQZYx5063gZY541xhQbY3KP2nbcY2Qcf/Z+5jYaY0b6ua7HjTHbvPt+wxgT792ebYypP+rYPennuk743hljfuw9XtuNMVf5qq6T1PbqUXXtM8as9273yzE7ST749jNmre2QP0AosBvoA0QAG4AhLtWSBoz03u8K7ACGAD8DfujycdoHJB2z7TfAw977DwOPufw+FgK93DpewCXASCD3VMcImAy8BxhgLLDSz3VNAMK89x87qq7sox/nwvE67nvn/XewAegC9Pb+mw31Z23H/P13wE/8ecxOkg8+/Yx15Jb7GGCXtXaPtbYJeAWY7kYh1toCa+067/1qYCuQ7kYtp2k6MNt7fzZwjXulcAWw21p7tlcot5m19mOg7JjNJzpG04HnrWMFEG+MSfNXXdbahdbaFu+vK4AMX+z7TOs6ienAK9baRmvtXmAXzr9dv9dmjDHATOBlX+3/BDWdKB98+hnryOGeDhw46veDBECgGmOygRHASu+m73q/Wj3r7+4PLwssNMasNcbM8m7rYa0t8N4vBHq4UNcRN/HVf2xuH68jTnSMAulzdzdOC++I3saYz40xS40x41yo53jvXSAdr3FAkbV251Hb/HrMjskHn37GOnK4BxxjTCwwF3jQWlsF/A3oCwwHCnC+EvrbxdbakcAk4H5jzCVH/9E63wNdGQ9rjIkApgH/8G4KhOP1NW4eoxMxxjwKtAAveTcVAFnW2hHA94E5xpg4P5YUkO/dMW7mqw0Jvx6z4+TDF3zxGevI4X4IyDzq9wzvNlcYY8Jx3riXrLWvA1hri6y1rdZaD/A0Pvw6eiLW2kPe22LgDW8NRUe+5nlvi/1dl9ckYJ21tshbo+vH6ygnOkauf+6MMXcCU4FbvKGAt9uj1Ht/LU7f9gB/1XSS98714wVgjAkDZgCvHtnmz2N2vHzAx5+xjhzuq4H+xpje3hbgTcDbbhTi7ct7Bthqrf39UduP7ie7Fsg99rk+rivGGNP1yH2ck3G5OMfpDu/D7gDe8mddR/lKS8rt43WMEx2jt4HbvSMaxgKVR3219jljzETgIWCatbbuqO3JxphQ7/0+QH9gjx/rOtF79zZwkzGmizGmt7euVf6q6yjjgW3W2oNHNvjrmJ0oH/D1Z8zXZ4p9+YNzVnkHzv9xH3WxjotxvlJtBNZ7fyYDLwCbvNvfBtL8XFcfnJEKG4DNR44RkAgsBnYCHwAJLhyzGKAU6HbUNleOF87/YAqAZpz+zXtOdIxwRjD8j/cztwkY7ee6duH0xx75nD3pfex13vd4PbAOuNrPdZ3wvQMe9R6v7cAkf7+X3u3PAfcd81i/HLOT5INPP2OafkBEJAh15G4ZERE5AYW7iEgQUriLiAQhhbuISBBSuIuIBCGFu4hIEFK4i4gEof8PU/okz47iNEMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(model=rnn_model_6 X_train=X_train, y_train=y_train, validation_split=0.1, patience=2, epochs=200, batch_size=16)\n",
    "\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7357fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    718.29187\n",
       "dtype: float32"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_6.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d758d",
   "metadata": {},
   "source": [
    "### Train model 7, GRU X 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Architecture\n",
    "\"\"\"   - 7th model layers architecture\n",
    "> GRU X 2\n",
    "\"\"\"\n",
    "rnn_model_7 = Sequential()\n",
    "rnn_model_7.add(normalizer) # Using the Normalization layer to standardize the datapoints during the forward pass\n",
    "# Input len(train) (input_shape=(?,?))\n",
    "rnn_model_7.add(GRU(units=30, activation='tanh', return_sequences=True, input_shape=(None, X_train.shape[1], X_train.shape[2])))  ## , input_shape=(?,?))) without a Normalizer layer\n",
    "rnn_model_7.add(GRU(units=20, activation='tanh', return_sequences=True))\n",
    "rnn_model_7.add(Dense(10, activation = 'relu')) ## add 1 or more 'relu' layers\n",
    "# Output 10 only, no more RNN just dropout()\n",
    "# rnn_model_3.add(layers.Dropout(0.3)) ## if RNN model over-fit\n",
    "rnn_model_7.add(Dense(n_pred, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87bb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  (None, None, 20)         41        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, None, 30)          4680      \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, None, 20)          3120      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, None, 10)          210       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, None, 1)           11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,062\n",
      "Trainable params: 8,021\n",
      "Non-trainable params: 41\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. Compiling with 'rmsprop' rather than 'adam' (recommended)\n",
    "optimizer = RMSprop(\n",
    "                learning_rate=0.001,\n",
    "                rho=0.9,\n",
    "                momentum=0.0,\n",
    "                epsilon=1e-07,\n",
    "                centered=False\n",
    "            )\n",
    "rnn_model_7.compile(loss='mse',\n",
    "              optimizer= optimizer, # optimizer='rmsprop'    <- adapt learning rate\n",
    "                 metrics='mape')  # Recommended optimizer for RNNs\n",
    "rnn_model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345f3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 6s 192ms/step - loss: 6508340736.0000 - mape: 99.9999 - val_loss: 6164800000.0000 - val_mape: 100.0000\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 2s 128ms/step - loss: 6508286464.0000 - mape: 99.9993 - val_loss: 6164745728.0000 - val_mape: 99.9988\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 1s 123ms/step - loss: 6508220928.0000 - mape: 99.9987 - val_loss: 6164642816.0000 - val_mape: 99.9978\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6508121600.0000 - mape: 99.9980 - val_loss: 6164547072.0000 - val_mape: 99.9965\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 6508022784.0000 - mape: 99.9972 - val_loss: 6164463616.0000 - val_mape: 99.9954\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 1s 125ms/step - loss: 6507936768.0000 - mape: 99.9965 - val_loss: 6164393984.0000 - val_mape: 99.9944\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 1s 120ms/step - loss: 6507857920.0000 - mape: 99.9956 - val_loss: 6164306944.0000 - val_mape: 99.9925\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6507777024.0000 - mape: 99.9945 - val_loss: 6164233728.0000 - val_mape: 99.9910\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 6507706368.0000 - mape: 99.9937 - val_loss: 6164163584.0000 - val_mape: 99.9897\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6507638272.0000 - mape: 99.9929 - val_loss: 6164096000.0000 - val_mape: 99.9884\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6507570176.0000 - mape: 99.9922 - val_loss: 6164026880.0000 - val_mape: 99.9870\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 2s 170ms/step - loss: 6507495424.0000 - mape: 99.9912 - val_loss: 6163943424.0000 - val_mape: 99.9850\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6507414016.0000 - mape: 99.9899 - val_loss: 6163870720.0000 - val_mape: 99.9835\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6507338752.0000 - mape: 99.9889 - val_loss: 6163795968.0000 - val_mape: 99.9821\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6507260928.0000 - mape: 99.9880 - val_loss: 6163718144.0000 - val_mape: 99.9805\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6507180032.0000 - mape: 99.9870 - val_loss: 6163637248.0000 - val_mape: 99.9790\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6507097088.0000 - mape: 99.9861 - val_loss: 6163557376.0000 - val_mape: 99.9774\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6507014144.0000 - mape: 99.9851 - val_loss: 6163469312.0000 - val_mape: 99.9758\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6506925056.0000 - mape: 99.9840 - val_loss: 6163384832.0000 - val_mape: 99.9740\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 3s 234ms/step - loss: 6506835456.0000 - mape: 99.9829 - val_loss: 6163297280.0000 - val_mape: 99.9722\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6506741760.0000 - mape: 99.9801 - val_loss: 6163199488.0000 - val_mape: 99.9594\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 6506641920.0000 - mape: 99.9777 - val_loss: 6163105280.0000 - val_mape: 99.9525\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6506505728.0000 - mape: 99.9734 - val_loss: 6162980864.0000 - val_mape: 99.9473\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 2s 179ms/step - loss: 6506353664.0000 - mape: 99.9689 - val_loss: 6162877952.0000 - val_mape: 99.9446\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6506243584.0000 - mape: 99.9672 - val_loss: 6162772992.0000 - val_mape: 99.9415\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 6506132992.0000 - mape: 99.9655 - val_loss: 6162663936.0000 - val_mape: 99.9382\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6506018816.0000 - mape: 99.9635 - val_loss: 6162557440.0000 - val_mape: 99.9345\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 3s 218ms/step - loss: 6505903104.0000 - mape: 99.9615 - val_loss: 6162443264.0000 - val_mape: 99.9306\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 2s 208ms/step - loss: 6505783296.0000 - mape: 99.9595 - val_loss: 6162327040.0000 - val_mape: 99.9267\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6505659904.0000 - mape: 99.9574 - val_loss: 6162213888.0000 - val_mape: 99.9225\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6505538048.0000 - mape: 99.9553 - val_loss: 6162095104.0000 - val_mape: 99.9183\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6505414144.0000 - mape: 99.9531 - val_loss: 6161979392.0000 - val_mape: 99.9137\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6505290752.0000 - mape: 99.9474 - val_loss: 6161855488.0000 - val_mape: 99.9095\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6505157120.0000 - mape: 99.9453 - val_loss: 6161723904.0000 - val_mape: 99.8729\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6505018880.0000 - mape: 99.9408 - val_loss: 6161593856.0000 - val_mape: 99.8648\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6504882688.0000 - mape: 99.9380 - val_loss: 6161462272.0000 - val_mape: 99.8589\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6504742912.0000 - mape: 99.9357 - val_loss: 6161327616.0000 - val_mape: 99.8528\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6504601088.0000 - mape: 99.9332 - val_loss: 6161193472.0000 - val_mape: 99.8471\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6504458240.0000 - mape: 99.9305 - val_loss: 6161055744.0000 - val_mape: 99.8412\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6504311808.0000 - mape: 99.9277 - val_loss: 6160918016.0000 - val_mape: 99.8353\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6504164864.0000 - mape: 99.9251 - val_loss: 6160771072.0000 - val_mape: 99.8290\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6504010240.0000 - mape: 99.9226 - val_loss: 6160622592.0000 - val_mape: 99.8227\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6503854592.0000 - mape: 99.9198 - val_loss: 6160474112.0000 - val_mape: 99.8165\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6503698944.0000 - mape: 99.9168 - val_loss: 6160325632.0000 - val_mape: 99.8102\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6503539200.0000 - mape: 99.9143 - val_loss: 6160172032.0000 - val_mape: 99.8037\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6503376896.0000 - mape: 99.9113 - val_loss: 6160019456.0000 - val_mape: 99.7973\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6503216128.0000 - mape: 99.9083 - val_loss: 6159863296.0000 - val_mape: 99.7908\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6503049216.0000 - mape: 99.9054 - val_loss: 6159702528.0000 - val_mape: 99.7840\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6502880256.0000 - mape: 99.9020 - val_loss: 6159540224.0000 - val_mape: 99.7772\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6502709248.0000 - mape: 99.8991 - val_loss: 6159379456.0000 - val_mape: 99.7704\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6502535168.0000 - mape: 99.8963 - val_loss: 6159205376.0000 - val_mape: 99.7631\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6502355456.0000 - mape: 99.8926 - val_loss: 6159033344.0000 - val_mape: 99.7559\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6502174720.0000 - mape: 99.8896 - val_loss: 6158865408.0000 - val_mape: 99.7488\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6501993472.0000 - mape: 99.8865 - val_loss: 6158688768.0000 - val_mape: 99.7414\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6501807104.0000 - mape: 99.8833 - val_loss: 6158511104.0000 - val_mape: 99.7340\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6501619200.0000 - mape: 99.8799 - val_loss: 6158329344.0000 - val_mape: 99.7263\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 6501428224.0000 - mape: 99.8762 - val_loss: 6158150656.0000 - val_mape: 99.7188\n",
      "Epoch 58/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6501238272.0000 - mape: 99.8730 - val_loss: 6157965824.0000 - val_mape: 99.7110\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6501043200.0000 - mape: 99.8695 - val_loss: 6157777920.0000 - val_mape: 99.7032\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6500845056.0000 - mape: 99.8660 - val_loss: 6157587456.0000 - val_mape: 99.6951\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6500642304.0000 - mape: 99.8620 - val_loss: 6157396992.0000 - val_mape: 99.6872\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6500440064.0000 - mape: 99.8587 - val_loss: 6157194240.0000 - val_mape: 99.6787\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6500231168.0000 - mape: 99.8548 - val_loss: 6156998144.0000 - val_mape: 99.6704\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6500022784.0000 - mape: 99.8511 - val_loss: 6156804096.0000 - val_mape: 99.6622\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6499815424.0000 - mape: 99.8474 - val_loss: 6156597248.0000 - val_mape: 99.6535\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6499598336.0000 - mape: 99.8438 - val_loss: 6156390400.0000 - val_mape: 99.6449\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6499384320.0000 - mape: 99.8394 - val_loss: 6156190208.0000 - val_mape: 99.6365\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6499167232.0000 - mape: 99.8360 - val_loss: 6155981824.0000 - val_mape: 99.6277\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6498948608.0000 - mape: 99.8315 - val_loss: 6155774976.0000 - val_mape: 99.6190\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 2s 204ms/step - loss: 6498726912.0000 - mape: 99.8281 - val_loss: 6155556352.0000 - val_mape: 99.6098\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6498498048.0000 - mape: 99.8242 - val_loss: 6155334656.0000 - val_mape: 99.6005\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 6498265600.0000 - mape: 99.8196 - val_loss: 6155113472.0000 - val_mape: 99.5912\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6498034176.0000 - mape: 99.8155 - val_loss: 6154892288.0000 - val_mape: 99.5820\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 2s 208ms/step - loss: 6497800704.0000 - mape: 99.8113 - val_loss: 6154672128.0000 - val_mape: 99.5727\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 6497565184.0000 - mape: 99.8069 - val_loss: 6154451456.0000 - val_mape: 99.5634\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 3s 211ms/step - loss: 6497328128.0000 - mape: 99.8029 - val_loss: 6154214400.0000 - val_mape: 99.5535\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 6497083392.0000 - mape: 99.7986 - val_loss: 6153983488.0000 - val_mape: 99.5437\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 2s 205ms/step - loss: 6496836608.0000 - mape: 99.7947 - val_loss: 6153745408.0000 - val_mape: 99.5337\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6496588800.0000 - mape: 99.7898 - val_loss: 6153518080.0000 - val_mape: 99.5242\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6496348672.0000 - mape: 99.7856 - val_loss: 6153284096.0000 - val_mape: 99.5143\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 2s 200ms/step - loss: 6496097280.0000 - mape: 99.7812 - val_loss: 6153036288.0000 - val_mape: 99.5039\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6495840768.0000 - mape: 99.7761 - val_loss: 6152793088.0000 - val_mape: 99.4938\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6495583232.0000 - mape: 99.7715 - val_loss: 6152543744.0000 - val_mape: 99.4833\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6495321088.0000 - mape: 99.7672 - val_loss: 6152305664.0000 - val_mape: 99.4733\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6495067136.0000 - mape: 99.7626 - val_loss: 6152055296.0000 - val_mape: 99.4627\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 2s 176ms/step - loss: 6494800384.0000 - mape: 99.7582 - val_loss: 6151792128.0000 - val_mape: 99.4517\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6494531072.0000 - mape: 99.7531 - val_loss: 6151544832.0000 - val_mape: 99.4413\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6494263808.0000 - mape: 99.7482 - val_loss: 6151287808.0000 - val_mape: 99.4305\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6493992960.0000 - mape: 99.7432 - val_loss: 6151025152.0000 - val_mape: 99.4194\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6493714432.0000 - mape: 99.7383 - val_loss: 6150759936.0000 - val_mape: 99.4083\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6493437952.0000 - mape: 99.7332 - val_loss: 6150505472.0000 - val_mape: 99.3976\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6493167104.0000 - mape: 99.7287 - val_loss: 6150234112.0000 - val_mape: 99.3862\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6492880384.0000 - mape: 99.7234 - val_loss: 6149958656.0000 - val_mape: 99.3746\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6492593152.0000 - mape: 99.7181 - val_loss: 6149687296.0000 - val_mape: 99.3632\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 2s 172ms/step - loss: 6492306944.0000 - mape: 99.7134 - val_loss: 6149424128.0000 - val_mape: 99.3521\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6492025344.0000 - mape: 99.7084 - val_loss: 6149138944.0000 - val_mape: 99.3401\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6491725312.0000 - mape: 99.7026 - val_loss: 6148849664.0000 - val_mape: 99.3280\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6491425280.0000 - mape: 99.6973 - val_loss: 6148572160.0000 - val_mape: 99.3163\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6491128320.0000 - mape: 99.6926 - val_loss: 6148282368.0000 - val_mape: 99.3041\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6490828800.0000 - mape: 99.6863 - val_loss: 6148003328.0000 - val_mape: 99.2924\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6490530304.0000 - mape: 99.6818 - val_loss: 6147706880.0000 - val_mape: 99.2799\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6490221056.0000 - mape: 99.6758 - val_loss: 6147425792.0000 - val_mape: 99.2681\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6489919488.0000 - mape: 99.6707 - val_loss: 6147137024.0000 - val_mape: 99.2560\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6489612288.0000 - mape: 99.6651 - val_loss: 6146836992.0000 - val_mape: 99.2433\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6489298432.0000 - mape: 99.6597 - val_loss: 6146542592.0000 - val_mape: 99.2310\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6488986112.0000 - mape: 99.6542 - val_loss: 6146246144.0000 - val_mape: 99.2185\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6488670720.0000 - mape: 99.6485 - val_loss: 6145934336.0000 - val_mape: 99.2054\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6488348160.0000 - mape: 99.6426 - val_loss: 6145631744.0000 - val_mape: 99.1926\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 6488026112.0000 - mape: 99.6363 - val_loss: 6145320448.0000 - val_mape: 99.1796\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 6487696896.0000 - mape: 99.6308 - val_loss: 6145006592.0000 - val_mape: 99.1663\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6487367680.0000 - mape: 99.6247 - val_loss: 6144690688.0000 - val_mape: 99.1531\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 3s 224ms/step - loss: 6487036416.0000 - mape: 99.6187 - val_loss: 6144380928.0000 - val_mape: 99.1400\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 2s 207ms/step - loss: 6486709760.0000 - mape: 99.6124 - val_loss: 6144065536.0000 - val_mape: 99.1268\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 6486368256.0000 - mape: 99.6075 - val_loss: 6143734784.0000 - val_mape: 99.1129\n",
      "Epoch 115/200\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 6486023680.0000 - mape: 99.6012 - val_loss: 6143398912.0000 - val_mape: 99.0988\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 3s 220ms/step - loss: 6485678592.0000 - mape: 99.5951 - val_loss: 6143091200.0000 - val_mape: 99.0858\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6485347328.0000 - mape: 99.5890 - val_loss: 6142763008.0000 - val_mape: 99.0720\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6484999680.0000 - mape: 99.5825 - val_loss: 6142427136.0000 - val_mape: 99.0579\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6484645376.0000 - mape: 99.5769 - val_loss: 6142093312.0000 - val_mape: 99.0438\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 2s 203ms/step - loss: 6484298752.0000 - mape: 99.5694 - val_loss: 6141759488.0000 - val_mape: 99.0298\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 6483947008.0000 - mape: 99.5636 - val_loss: 6141423104.0000 - val_mape: 99.0156\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 3s 215ms/step - loss: 6483587072.0000 - mape: 99.5571 - val_loss: 6141078016.0000 - val_mape: 99.0011\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6483226112.0000 - mape: 99.5513 - val_loss: 6140733440.0000 - val_mape: 98.9866\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 6482865664.0000 - mape: 99.5443 - val_loss: 6140393472.0000 - val_mape: 98.9723\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6482504192.0000 - mape: 99.5381 - val_loss: 6140046336.0000 - val_mape: 98.9577\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6482135040.0000 - mape: 99.5315 - val_loss: 6139686400.0000 - val_mape: 98.9425\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6481763840.0000 - mape: 99.5241 - val_loss: 6139338752.0000 - val_mape: 98.9279\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6481391616.0000 - mape: 99.5186 - val_loss: 6138976768.0000 - val_mape: 98.9127\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6481013760.0000 - mape: 99.5111 - val_loss: 6138623488.0000 - val_mape: 98.8978\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 2s 171ms/step - loss: 6480641024.0000 - mape: 99.5044 - val_loss: 6138269696.0000 - val_mape: 98.8829\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6480264704.0000 - mape: 99.4981 - val_loss: 6137910784.0000 - val_mape: 98.8678\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6479885824.0000 - mape: 99.4907 - val_loss: 6137543168.0000 - val_mape: 98.8523\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6479494656.0000 - mape: 99.4842 - val_loss: 6137167872.0000 - val_mape: 98.8365\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6479104512.0000 - mape: 99.4775 - val_loss: 6136788992.0000 - val_mape: 98.8206\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6478711808.0000 - mape: 99.4695 - val_loss: 6136430592.0000 - val_mape: 98.8055\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6478324736.0000 - mape: 99.4633 - val_loss: 6136050688.0000 - val_mape: 98.7895\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6477929472.0000 - mape: 99.4555 - val_loss: 6135675392.0000 - val_mape: 98.7737\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6477530112.0000 - mape: 99.4483 - val_loss: 6135296000.0000 - val_mape: 98.7577\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6477130240.0000 - mape: 99.4417 - val_loss: 6134906368.0000 - val_mape: 98.7413\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6476723200.0000 - mape: 99.4340 - val_loss: 6134532096.0000 - val_mape: 98.7256\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 2s 181ms/step - loss: 6476329472.0000 - mape: 99.4268 - val_loss: 6134149120.0000 - val_mape: 98.7094\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6475919360.0000 - mape: 99.4200 - val_loss: 6133744640.0000 - val_mape: 98.6924\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6475498496.0000 - mape: 99.4128 - val_loss: 6133349376.0000 - val_mape: 98.6758\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 2s 199ms/step - loss: 6475084288.0000 - mape: 99.4054 - val_loss: 6132956160.0000 - val_mape: 98.6592\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 6474669056.0000 - mape: 99.3976 - val_loss: 6132567040.0000 - val_mape: 98.6428\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6474255360.0000 - mape: 99.3904 - val_loss: 6132171776.0000 - val_mape: 98.6262\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 2s 202ms/step - loss: 6473837568.0000 - mape: 99.3823 - val_loss: 6131761664.0000 - val_mape: 98.6089\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6473412096.0000 - mape: 99.3750 - val_loss: 6131362816.0000 - val_mape: 98.5921\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 3s 216ms/step - loss: 6472987648.0000 - mape: 99.3669 - val_loss: 6130956288.0000 - val_mape: 98.5749\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6472557568.0000 - mape: 99.3603 - val_loss: 6130549248.0000 - val_mape: 98.5578\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 3s 242ms/step - loss: 6472132096.0000 - mape: 99.3515 - val_loss: 6130147328.0000 - val_mape: 98.5409\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 3s 211ms/step - loss: 6471698944.0000 - mape: 99.3448 - val_loss: 6129713152.0000 - val_mape: 98.5226\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 3s 222ms/step - loss: 6471254528.0000 - mape: 99.3361 - val_loss: 6129302016.0000 - val_mape: 98.5052\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 3s 223ms/step - loss: 6470817280.0000 - mape: 99.3281 - val_loss: 6128883200.0000 - val_mape: 98.4876\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 3s 220ms/step - loss: 6470372864.0000 - mape: 99.3205 - val_loss: 6128456704.0000 - val_mape: 98.4697\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 3s 234ms/step - loss: 6469926400.0000 - mape: 99.3127 - val_loss: 6128030720.0000 - val_mape: 98.4517\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 3s 253ms/step - loss: 6469479424.0000 - mape: 99.3053 - val_loss: 6127604736.0000 - val_mape: 98.4337\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 3s 257ms/step - loss: 6469031424.0000 - mape: 99.2967 - val_loss: 6127178240.0000 - val_mape: 98.4158\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 3s 208ms/step - loss: 6468581376.0000 - mape: 99.2887 - val_loss: 6126760448.0000 - val_mape: 98.3982\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 3s 222ms/step - loss: 6468137472.0000 - mape: 99.2809 - val_loss: 6126326784.0000 - val_mape: 98.3799\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6467680768.0000 - mape: 99.2721 - val_loss: 6125899776.0000 - val_mape: 98.3619\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 3s 217ms/step - loss: 6467228160.0000 - mape: 99.2648 - val_loss: 6125454848.0000 - val_mape: 98.3431\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6466763264.0000 - mape: 99.2553 - val_loss: 6125008384.0000 - val_mape: 98.3243\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 3s 217ms/step - loss: 6466293760.0000 - mape: 99.2467 - val_loss: 6124583936.0000 - val_mape: 98.3064\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 6465838080.0000 - mape: 99.2402 - val_loss: 6124129280.0000 - val_mape: 98.2873\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 3s 228ms/step - loss: 6465360896.0000 - mape: 99.2313 - val_loss: 6123678208.0000 - val_mape: 98.2682\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 6464889856.0000 - mape: 99.2221 - val_loss: 6123215360.0000 - val_mape: 98.2487\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 3s 215ms/step - loss: 6464412672.0000 - mape: 99.2134 - val_loss: 6122796032.0000 - val_mape: 98.2311\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 6463960576.0000 - mape: 99.2060 - val_loss: 6122342912.0000 - val_mape: 98.2119\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6463479808.0000 - mape: 99.1978 - val_loss: 6121879552.0000 - val_mape: 98.1924\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6462995968.0000 - mape: 99.1884 - val_loss: 6121416192.0000 - val_mape: 98.1729\n",
      "Epoch 172/200\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 6462513152.0000 - mape: 99.1788 - val_loss: 6120978432.0000 - val_mape: 98.1544\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6462044672.0000 - mape: 99.1713 - val_loss: 6120514560.0000 - val_mape: 98.1348\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6461553152.0000 - mape: 99.1620 - val_loss: 6120049664.0000 - val_mape: 98.1152\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6461061120.0000 - mape: 99.1543 - val_loss: 6119570432.0000 - val_mape: 98.0951\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 6460563456.0000 - mape: 99.1446 - val_loss: 6119093760.0000 - val_mape: 98.0749\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6460056576.0000 - mape: 99.1356 - val_loss: 6118606848.0000 - val_mape: 98.0544\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 2s 173ms/step - loss: 6459552768.0000 - mape: 99.1270 - val_loss: 6118130688.0000 - val_mape: 98.0343\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 2s 188ms/step - loss: 6459054080.0000 - mape: 99.1177 - val_loss: 6117667328.0000 - val_mape: 98.0147\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6458561024.0000 - mape: 99.1088 - val_loss: 6117209600.0000 - val_mape: 97.9954\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6458073088.0000 - mape: 99.1001 - val_loss: 6116715520.0000 - val_mape: 97.9746\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6457556480.0000 - mape: 99.0904 - val_loss: 6116232192.0000 - val_mape: 97.9542\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 2s 183ms/step - loss: 6457046528.0000 - mape: 99.0812 - val_loss: 6115746816.0000 - val_mape: 97.9337\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 6456534528.0000 - mape: 99.0720 - val_loss: 6115257344.0000 - val_mape: 97.9131\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 2s 177ms/step - loss: 6456018944.0000 - mape: 99.0633 - val_loss: 6114757632.0000 - val_mape: 97.8920\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 6455497216.0000 - mape: 99.0532 - val_loss: 6114275328.0000 - val_mape: 97.8716\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 6454986752.0000 - mape: 99.0450 - val_loss: 6113780736.0000 - val_mape: 97.8508\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6454462976.0000 - mape: 99.0353 - val_loss: 6113285120.0000 - val_mape: 97.8299\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 2s 174ms/step - loss: 6453937152.0000 - mape: 99.0260 - val_loss: 6112766976.0000 - val_mape: 97.8080\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6453396992.0000 - mape: 99.0159 - val_loss: 6112254976.0000 - val_mape: 97.7864\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 2s 191ms/step - loss: 6452864512.0000 - mape: 99.0060 - val_loss: 6111764480.0000 - val_mape: 97.7657\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 2s 185ms/step - loss: 6452339712.0000 - mape: 98.9964 - val_loss: 6111243264.0000 - val_mape: 97.7437\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 2s 184ms/step - loss: 6451791360.0000 - mape: 98.9878 - val_loss: 6110729216.0000 - val_mape: 97.7220\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6451253248.0000 - mape: 98.9784 - val_loss: 6110217728.0000 - val_mape: 97.7004\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 2s 198ms/step - loss: 6450710016.0000 - mape: 98.9674 - val_loss: 6109691904.0000 - val_mape: 97.6782\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 2s 186ms/step - loss: 6450161664.0000 - mape: 98.9586 - val_loss: 6109177344.0000 - val_mape: 97.6565\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 2s 204ms/step - loss: 6449615360.0000 - mape: 98.9490 - val_loss: 6108666368.0000 - val_mape: 97.6349\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 2s 189ms/step - loss: 6449075712.0000 - mape: 98.9388 - val_loss: 6108140544.0000 - val_mape: 97.6127\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 2s 206ms/step - loss: 6448521216.0000 - mape: 98.9280 - val_loss: 6107608064.0000 - val_mape: 97.5902\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 3s 209ms/step - loss: 6447969792.0000 - mape: 98.9178 - val_loss: 6107094016.0000 - val_mape: 97.5685\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = train_rnn_model(model=rnn_model_7 X_train=X_train, y_train=y_train, validation_split=0.1, patience=2, epochs=200, batch_size=16)\n",
    "plt.plot(history.history['mape'])\n",
    "plt.plot(history.history['val_mape'])\n",
    "plt.show();\n",
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[166.99881]\n",
      "  [359.90118]\n",
      "  [420.147  ]\n",
      "  ...\n",
      "  [423.9484 ]\n",
      "  [423.94827]\n",
      "  [423.94815]]\n",
      "\n",
      " [[166.51483]\n",
      "  [359.48444]\n",
      "  [420.10287]\n",
      "  ...\n",
      "  [423.94675]\n",
      "  [423.94656]\n",
      "  [423.9465 ]]\n",
      "\n",
      " [[166.45361]\n",
      "  [359.4035 ]\n",
      "  [420.09558]\n",
      "  ...\n",
      "  [423.94693]\n",
      "  [423.94684]\n",
      "  [423.9467 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[166.50644]\n",
      "  [359.47742]\n",
      "  [420.10526]\n",
      "  ...\n",
      "  [423.94702]\n",
      "  [423.9469 ]\n",
      "  [423.9468 ]]\n",
      "\n",
      " [[166.41922]\n",
      "  [359.4327 ]\n",
      "  [420.09976]\n",
      "  ...\n",
      "  [423.94656]\n",
      "  [423.9465 ]\n",
      "  [423.94638]]\n",
      "\n",
      " [[166.41922]\n",
      "  [359.4327 ]\n",
      "  [420.09976]\n",
      "  ...\n",
      "  [423.94656]\n",
      "  [423.9465 ]\n",
      "  [423.94638]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(20, 61, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31858/3491075012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Distribution of the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 )\n\u001b[1;32m    671\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(20, 61, 1)"
     ]
    }
   ],
   "source": [
    "# 4. Evaluating\n",
    "# The prediction (one per sequence/city)\n",
    "y_pred = rnn_model_7.predict(X_test) \n",
    "print(y_pred.shape)\n",
    "# Distribution of the predictions\n",
    "pd.DataFrame(y_pred).mean().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "9c7d4b3da1e137aeabc4fd6b55dede4bb1c85dbdc50880a4fde3d2604903e3dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
